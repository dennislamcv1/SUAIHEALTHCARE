{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Dense Neural Network for Heart Disease Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time estimate: **30** minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "- Preprocess medical data by handling missing values, normalizing features, and encoding categorical variables for neural network training\n",
    "- Build a fully connected (dense) neural network capable of learning from patient data to predict heart disease presence\n",
    "- Train and evaluate the model using appropriate metrics, and visualize learning curves to assess model performance and improvement\n",
    "\n",
    "## What you will do in this lab\n",
    "\n",
    "In this hands-on lab, you will build your first neural network to predict heart disease using patient medical records. This practical exercise will introduce you to the complete machine learning workflow, from data loading through model evaluation.\n",
    "\n",
    "You will:\n",
    "\n",
    "- Explore and analyze the Heart Disease UCI dataset containing clinical parameters\n",
    "- Preprocess medical data using standardization techniques\n",
    "- Design and implement a multi-layer neural network architecture\n",
    "- Train the model using backpropagation and evaluate its performance\n",
    "- Interpret results using classification reports, confusion matrices, and learning curves\n",
    "\n",
    "## Overview\n",
    "\n",
    "Heart disease remains one of the leading causes of death worldwide, making early detection critical for effective treatment. Machine learning, particularly neural networks, has shown remarkable success in medical diagnosis by identifying complex patterns in patient data that might not be immediately apparent to human observers.\n",
    "\n",
    "In this lab, you will build a dense (fully connected) neural network to predict the presence of heart disease based on 13 clinical features such as age, blood pressure, cholesterol levels, and electrocardiogram results. Dense neural networks are particularly well-suited for this task because they can learn non-linear relationships between multiple features simultaneously.\n",
    "\n",
    "By the end of this lab, you will understand how to apply neural networks to real-world medical data and interpret the results in a clinically meaningful way. This foundational knowledge will prepare you for more advanced deep learning applications in healthcare and other domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset\n",
    "\n",
    "This dataset contains medical records for heart disease diagnosis and prediction. It is widely used in cardiovascular research and machine learning applications to predict the presence of heart disease in patients based on various clinical parameters.\n",
    "\n",
    "### Dataset overview\n",
    "\n",
    "The Cleveland Heart Disease Database is one of the most widely-used datasets for heart disease prediction research. Originally collected at the Cleveland Clinic Foundation, it contains comprehensive medical information from patients undergoing cardiovascular evaluations. The dataset has been extensively validated and is considered a benchmark for developing and testing machine learning models in medical diagnostics.\n",
    "\n",
    "The dataset includes 303 patient records with 14 attributes (13 features and 1 target variable). These features represent a combination of demographic information, symptoms, clinical test results, and diagnostic measurements that physicians commonly use to assess cardiovascular health. The target variable indicates whether heart disease is present, making this a binary classification problem.\n",
    "\n",
    "### Column descriptions\n",
    "\n",
    "1. **age** - Age of the patient in years (range: 29-77 years)\n",
    "2. **sex** - Gender of the patient (0 = Female, 1 = Male)\n",
    "3. **cp** - Chest pain type: 0 = typical angina, 1 = atypical angina, 2 = non-anginal pain, 3 = asymptomatic\n",
    "4. **trestbps** - Resting blood pressure measured in mm Hg on admission to the hospital (normal range: 90-140 mm Hg)\n",
    "5. **chol** - Serum cholesterol level in mg/dl (desirable: < 200 mg/dl)\n",
    "6. **fbs** - Fasting blood sugar > 120 mg/dl (0 = No, 1 = Yes); indicates potential diabetes\n",
    "7. **restecg** - Resting electrocardiographic results: 0 = normal, 1 = ST-T wave abnormality, 2 = left ventricular hypertrophy\n",
    "8. **thalach** - Maximum heart rate achieved during exercise stress test (normal range: 60-100 bpm at rest)\n",
    "9. **exang** - Exercise-induced angina (chest pain triggered by physical activity): 0 = No, 1 = Yes\n",
    "10. **oldpeak** - ST depression induced by exercise relative to rest (measures heart stress during exercise)\n",
    "11. **slope** - Slope of the peak exercise ST segment: 0 = upsloping (better), 1 = flat, 2 = downsloping (worse)\n",
    "12. **ca** - Number of major blood vessels (0-3) colored by fluoroscopy (fewer vessels visible indicates potential blockage)\n",
    "13. **thal** - Thalassemia/Thallium stress test result: 1 = normal blood flow, 2 = fixed defect, 3 = reversible defect\n",
    "14. **target** - Heart disease diagnosis: 0 = No disease (< 50% diameter narrowing), 1 = Disease present (> 50% diameter narrowing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Installing required libraries\n",
    "\n",
    "The following libraries are required to run this lab. If you are running this notebook in a local environment, you may need to install these libraries using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas matplotlib scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numerical computing and data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import scikit-learn utilities for data preparation and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Import TensorFlow and Keras for building neural networks\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the patient dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Heart Disease Dataset from CSV file\n",
    "data = pd.read_csv(\"https://advanced-machine-learning-for-medical-data-8e1579.gitlab.io/labs/lab7/heart.csv\")\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"This means {data.shape[0]} patients and {data.shape[1]} columns (features + target)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows to understand the data structure\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the target variable (heart disease presence)\n",
    "print(\"Target variable distribution:\")\n",
    "print(data[\"target\"].value_counts())\n",
    "\n",
    "# Display class proportions as percentages\n",
    "print(\"\\nClass proportions:\")\n",
    "print(data[\"target\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 3: Identify input features (X) and target variable (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "# X contains all columns except 'target' - these are the input variables\n",
    "# y contains only the 'target' column - this is what you want to predict\n",
    "X = data.drop(columns=[\"target\"])\n",
    "y = data[\"target\"]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split the data into Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "# test_size=0.2 means 20% of data goes to testing, 80% to training\n",
    "# random_state=42 ensures reproducibility (same split every time you run this code)\n",
    "# stratify=y maintains the same class proportion in both train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,        # 20% for testing\n",
    "    random_state=42,      # for reproducibility\n",
    "    stratify=y            # maintain class balance in splits\n",
    ")\n",
    "\n",
    "print(\"Data split completed!\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining proportion: {X_train.shape[0] / len(data) * 100:.1f}%\")\n",
    "print(f\"Testing proportion: {X_test.shape[0] / len(data) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Standardize the features\n",
    "\n",
    "**Feature standardization** is a crucial preprocessing step for neural networks. Different features in the dataset have different scales (for example, age ranges from 29-77, while the sex variable is only 0 or 1). Neural networks perform better when all features are on a similar scale.\n",
    "\n",
    "**StandardScaler** transforms each feature to have:\n",
    "- A mean of 0 (centered around zero)\n",
    "- A standard deviation of 1 (unit variance)\n",
    "\n",
    "This process is called **Z-score normalization** and is calculated as: `z = (x - mean) / standard_deviation`\n",
    "\n",
    "By standardizing features, you ensure that no single feature dominates the learning process simply because it has larger values. This is especially important for algorithms like neural networks that use gradient descent optimization.\n",
    "\n",
    "**Important:** You fit the scaler only on the training data, then apply the same transformation to both training and testing data. This prevents data leakage from the test set into the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler object to standardize features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on training data and transform it\n",
    "# fit_transform() calculates mean and std from training data, then standardizes it\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform test data using the same scaler parameters (mean and std from training data)\n",
    "# This ensures consistent scaling between training and testing\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature standardization completed!\")\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first row of standardized training data to see the transformation\n",
    "# Notice how all values are now centered around 0 with similar scales\n",
    "print(\"First patient's standardized features:\")\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dense neural network using Keras Sequential API\n",
    "model = keras.Sequential([\n",
    "    # Input layer: accepts 13 features (patient's clinical measurements)\n",
    "    layers.Input(shape=(13,)),   \n",
    "    \n",
    "    # First hidden layer: 16 neurons with ReLU activation\n",
    "    # ReLU helps the model learn complex patterns by focusing on positive signals\n",
    "    layers.Dense(16, activation=\"relu\"),       \n",
    "    \n",
    "    # Second hidden layer: 8 neurons with ReLU activation\n",
    "    # Further refines the learned patterns\n",
    "    layers.Dense(8, activation=\"relu\"),        \n",
    "    \n",
    "    # Output layer: 1 neuron with sigmoid activation\n",
    "    # Sigmoid produces a probability between 0 and 1 for binary classification\n",
    "    layers.Dense(1, activation=\"sigmoid\")      \n",
    "])\n",
    "\n",
    "print(\"Neural network architecture created!\")\n",
    "print(\"Architecture: Input (13 features) → 16 neurons → 8 neurons → 1 output (probability)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with loss function, optimizer, and metrics\n",
    "model.compile(\n",
    "    # Loss function: measures prediction error for binary classification\n",
    "    loss=\"binary_crossentropy\",\n",
    "    \n",
    "    # Optimizer: algorithm that updates model weights to minimize loss\n",
    "    optimizer=\"adam\",\n",
    "    \n",
    "    # Metrics: tracks accuracy (percentage of correct predictions)\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")\n",
    "print(\"\\nConfiguration:\")\n",
    "print(\"- Loss function: binary_crossentropy (measures prediction errors)\")\n",
    "print(\"- Optimizer: adam (adapts learning rate for efficient training)\")\n",
    "print(\"- Metrics: accuracy (tracks percentage of correct predictions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed model architecture summary\n",
    "print(\"Model Architecture Summary:\")\n",
    "print(\"=\"*60)\n",
    "model.summary()\n",
    "\n",
    "print(\"\\nThe summary shows:\")\n",
    "print(\"- Layer types and output shapes\")\n",
    "print(\"- Number of trainable parameters (weights) in each layer\")\n",
    "print(\"- Total parameters that will be learned during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network on the training data\n",
    "print(\"Starting model training...\")\n",
    "print(\"This may take a minute or two...\")\n",
    "\n",
    "# Train the model and store training history\n",
    "history = model.fit(\n",
    "    # Training features and labels\n",
    "    X_train, \n",
    "    y_train,\n",
    "    \n",
    "    # Validation data to monitor performance on unseen data\n",
    "    validation_data=(X_test, y_test),\n",
    "    \n",
    "    # Number of complete passes through the training data\n",
    "    epochs=50,\n",
    "    \n",
    "    # Number of samples processed before updating weights\n",
    "    batch_size=32,\n",
    "    \n",
    "    # Show progress bar during training\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Make predictions on test data\n",
    "\n",
    "After training, you use the model to make predictions on the test set. The neural network outputs probabilities between 0 and 1 for each patient. To convert these probabilities into binary predictions (0 = no disease, 1 = disease present), you use a threshold of 0.5:\n",
    "- If probability > 0.5, predict 1 (disease present)\n",
    "- If probability ≤ 0.5, predict 0 (no disease)\n",
    "\n",
    "This allows us to compare the model's predictions with the actual diagnoses and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set\n",
    "# The model outputs probabilities (values between 0 and 1) for each patient\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to binary predictions using 0.5 threshold\n",
    "# If probability > 0.5, predict 1 (disease present); otherwise predict 0 (no disease)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "print(\"Predictions generated!\")\n",
    "print(f\"Number of test samples: {y_pred.shape[0]}\")\n",
    "print(f\"\\nFirst 10 predictions: {y_pred.flatten()[:10]}\")\n",
    "print(f\"First 10 actual values: {y_test.values[:10]}\")\n",
    "print(f\"\\nCompare to see how well the model predicted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Print classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nMetric Definitions:\")\n",
    "print(\"- Precision: Percentage of positive predictions that were actually correct\")\n",
    "print(\"- Recall: Percentage of actual positive cases that were correctly identified\")\n",
    "print(\"- F1-score: Balanced measure combining precision and recall\")\n",
    "print(\"- Support: Number of actual samples in each class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Print confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"=\"*60)\n",
    "print(cm)\n",
    "print(\"\\nMatrix Layout:\")\n",
    "print(\"[[TN  FP]\")\n",
    "print(\" [FN  TP]]\")\n",
    "\n",
    "print(\"\\nDetailed Breakdown:\")\n",
    "print(f\"True Negatives (TN): {cm[0, 0]} - Correctly predicted no disease\")\n",
    "print(f\"False Positives (FP): {cm[0, 1]} - Incorrectly predicted disease (false alarm)\")\n",
    "print(f\"False Negatives (FN): {cm[1, 0]} - Incorrectly predicted no disease (missed diagnosis)\")\n",
    "print(f\"True Positives (TP): {cm[1, 1]} - Correctly predicted disease\")\n",
    "\n",
    "# Calculate overall accuracy from confusion matrix\n",
    "accuracy = (cm[0, 0] + cm[1, 1]) / cm.sum()\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Plot accuracy over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\", linewidth=2)\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\", linewidth=2)\n",
    "\n",
    "plt.title(\"Model Accuracy Over Epochs\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The accuracy plot shows how the model's performance improved during training.\")\n",
    "print(\"Both training and validation accuracy should increase over time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Plot training and validation loss over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot training loss\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\", linewidth=2)\n",
    "\n",
    "# Plot validation loss\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\", linewidth=2)\n",
    "\n",
    "plt.title(\"Model Loss Over Epochs\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.legend(loc='upper right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The loss plot shows how prediction error decreased during training.\")\n",
    "print(\"Both training and validation loss should decrease and stabilize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side plots for accuracy and loss\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Left plot: Accuracy\n",
    "axes[0].plot(history.history[\"accuracy\"], label=\"Training Accuracy\", linewidth=2)\n",
    "axes[0].plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\", linewidth=2)\n",
    "axes[0].set_title(\"Model Accuracy\", fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Accuracy\", fontsize=12)\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right plot: Loss\n",
    "axes[1].plot(history.history[\"loss\"], label=\"Training Loss\", linewidth=2)\n",
    "axes[1].plot(history.history[\"val_loss\"], label=\"Validation Loss\", linewidth=2)\n",
    "axes[1].set_title(\"Model Loss\", fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Loss\", fontsize=12)\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Combined visualization shows both accuracy improvement and loss reduction over training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final model performance on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Final Model Performance on Test Set:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(\"\\nThis accuracy represents the model's performance on completely unseen data.\")\n",
    "print(\"It's the best indicator of how the model would perform in real-world scenarios.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "In Step 6, you built a neural network. Given below is one more architecture for the neural network. This is a deeper network with more layers. Replace the existing neural network with this network, and observe the change in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the deeper network architecture variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a deeper neural network with dropout regularization\n",
    "model = keras.Sequential([\n",
    "    # Input layer: accepts 13 features\n",
    "    layers.Input(shape=(13,)),\n",
    "    \n",
    "    # First hidden layer: 32 neurons with ReLU activation\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),  # Drop 30% of neurons during training to prevent overfitting\n",
    "    \n",
    "    # Second hidden layer: 16 neurons with ReLU activation\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),  # Additional dropout for regularization\n",
    "    \n",
    "    # Third hidden layer: 8 neurons with ReLU activation\n",
    "    layers.Dense(8, activation=\"relu\"),\n",
    "    \n",
    "    # Output layer: 1 neuron with sigmoid activation\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have successfully completed this lab on building dense neural networks for heart disease prediction! Throughout this hands-on exercise, you gained practical experience with the complete machine learning workflow, from data preprocessing through model evaluation. You now understand how to design neural network architectures, train them using backpropagation, and interpret their performance using various evaluation metrics.\n",
    "\n",
    "The skills you developed in this lab form the foundation for more advanced deep learning applications in healthcare, finance, and many other domains. You've learned not just how to build neural networks, but also how to think critically about model performance, interpret results, and make data-driven decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "Ramesh Sannareddy\n",
    "\n",
    "Copyright © 2025 SkillUp. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
