{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFviLGIw39ZW"
   },
   "source": [
    "# Basic Supervised Model Implementation for Patient Risk Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time estimate: **45** minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2K4Lwx139ZZ"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    " - Prepare and split a clinical dataset for patient risk prediction\n",
    " - Train and evaluate a Random Forest model using key performance metrics\n",
    " - Interpret and visualize model results to assess clinical relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you will do in this lab\n",
    "\n",
    "In this lab, you will work with a real clinical dataset to build and test a simple patient risk prediction model.\n",
    "\n",
    "You will:\n",
    "\n",
    "- Explore the dataset and identify key features for prediction\n",
    "- Split the data into training and test sets to prepare for modeling\n",
    "- Train a Random Forest model to predict patient risk\n",
    "- Evaluate how well the model performs using key metrics and visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In healthcare, predicting patient risk is not just a technical exercise. It supports real clinical decisions. A good model helps identify who might need early intervention or additional monitoring, improving patient outcomes.\n",
    "\n",
    "You will use a Random Forest classifier for this task because it is well suited to healthcare data, which often involves many interdependent features and complex relationships. Instead of relying on a single decision tree that might overfit, a Random Forest builds many trees and aggregates their predictions. This ensemble approach reduces variance, improves generalization, and provides insights into which features influence predictions the most.\n",
    "\n",
    "Evaluating the model goes beyond accuracy. Metrics like sensitivity and specificity tell you whether the model correctly identifies high-risk patients and avoids false alarms. The confusion matrix helps visualize these trade-offs, showing whether the model’s predictions align with clinical priorities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oi8caFHd39ZZ"
   },
   "source": [
    "## About the dataset\n",
    "\n",
    "In this lab, you will use a dataset based on the Wisconsin Breast Cancer Dataset.\n",
    "\n",
    "### Dataset overview\n",
    "The Wisconsin Breast Cancer Dataset is a comprehensive collection of diagnostic measurements derived from digitized images of fine needle aspirate (FNA) of breast masses. This dataset is widely used in machine learning and medical research for binary classification tasks to predict whether a breast mass is benign or malignant.\n",
    "\n",
    "### Column descriptions\n",
    "\n",
    "1. **ID Number** - Unique identification number assigned to each patient sample for tracking and reference purposes\n",
    "\n",
    "2. **Diagnosis** - Binary classification of the tumor: M (Malignant) or B (Benign), representing the target variable for prediction\n",
    "\n",
    "3. **Radius Mean** - Mean of distances from center to points on the perimeter of the cell nucleus, measured in micrometers\n",
    "\n",
    "4. **Texture Mean** - Mean of standard deviation of gray-scale values in the nucleus image, representing surface texture variation\n",
    "\n",
    "5. **Perimeter Mean** - Mean perimeter measurement of the cell nucleus boundary, calculated in micrometers\n",
    "\n",
    "6. **Area Mean** - Mean area of the cell nucleus, measured in square micrometers\n",
    "\n",
    "7. **Smoothness Mean** - Mean of local variation in radius lengths, indicating the smoothness of the nucleus contour\n",
    "\n",
    "8. **Compactness Mean** - Mean compactness calculated as (perimeter² / area - 1.0), describing the shape regularity of the nucleus\n",
    "\n",
    "9. **Concavity Mean** - Mean severity of concave portions of the nucleus contour, representing indentations in the cell boundary\n",
    "\n",
    "10. **Concave Points Mean** - Mean number of concave portions of the nucleus contour, counting the frequency of indentations\n",
    "\n",
    "11. **Symmetry Mean** - Mean symmetry measurement of the cell nucleus, assessing bilateral similarity\n",
    "\n",
    "12. **Fractal Dimension Mean** - Mean fractal dimension calculated using \"coastline approximation\" method, representing complexity of the nucleus boundary\n",
    "\n",
    "13. **Radius SE** - Standard error of distances from center to points on the perimeter, indicating measurement variability\n",
    "\n",
    "14. **Texture SE** - Standard error of gray-scale value standard deviations, representing texture measurement precision\n",
    "\n",
    "15. **Perimeter SE** - Standard error of perimeter measurements, indicating boundary measurement variability\n",
    "\n",
    "16. **Area SE** - Standard error of area measurements, representing variability in nucleus size calculations\n",
    "\n",
    "17. **Smoothness SE** - Standard error of local radius length variations, indicating smoothness measurement precision\n",
    "\n",
    "18. **Compactness SE** - Standard error of compactness values, representing variability in shape regularity measurements\n",
    "\n",
    "19. **Concavity SE** - Standard error of concavity measurements, indicating precision of contour indentation assessments\n",
    "\n",
    "20. **Concave Points SE** - Standard error of concave point counts, representing variability in indentation frequency measurements\n",
    "\n",
    "21. **Symmetry SE** - Standard error of symmetry measurements, indicating precision of bilateral similarity assessments\n",
    "\n",
    "22. **Fractal Dimension SE** - Standard error of fractal dimension calculations, representing variability in boundary complexity measurements\n",
    "\n",
    "23. **Radius Worst** - Largest (worst) mean value for radius among all cells in the sample, representing maximum nucleus size\n",
    "\n",
    "24. **Texture Worst** - Largest (worst) mean value for texture among all cells in the sample, representing maximum surface variation\n",
    "\n",
    "25. **Perimeter Worst** - Largest (worst) mean value for perimeter among all cells in the sample, representing maximum boundary length\n",
    "\n",
    "26. **Area Worst** - Largest (worst) mean value for area among all cells in the sample, representing maximum nucleus coverage\n",
    "\n",
    "27. **Smoothness Worst** - Largest (worst) mean value for smoothness among all cells in the sample, representing maximum contour irregularity\n",
    "\n",
    "28. **Compactness Worst** - Largest (worst) mean value for compactness among all cells in the sample, representing maximum shape irregularity\n",
    "\n",
    "29. **Concavity Worst** - Largest (worst) mean value for concavity among all cells in the sample, representing maximum contour indentation severity\n",
    "\n",
    "30. **Concave Points Worst** - Largest (worst) mean value for concave points among all cells in the sample, representing maximum indentation frequency\n",
    "\n",
    "31. **Symmetry Worst** - Largest (worst) mean value for symmetry among all cells in the sample, representing maximum bilateral asymmetry\n",
    "\n",
    "32. **Fractal Dimension Worst** - Largest (worst) mean value for fractal dimension among all cells in the sample, representing maximum boundary complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyjm2rHq39Za"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZSLNZud39Za"
   },
   "source": [
    "### Installing required libraries\n",
    "\n",
    "The following libraries are required to run this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CoZK2MXo39Za"
   },
   "outputs": [],
   "source": [
    "# Install the Libraries required for this lab.\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Tp2qsVRLPgI"
   },
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUhUGk-y39Zb"
   },
   "source": [
    "### Importing required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "II2EOHb639Zb"
   },
   "outputs": [],
   "source": [
    "# Import pandas for data manipulation (like Excel for Python)\n",
    "import pandas as pd\n",
    "# Import numpy for numerical operations\n",
    "import numpy as np\n",
    "# Import machine learning tools from scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import our machine learning algorithm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Import evaluation metrics for measuring model performance\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    ")\n",
    "#import matplotlib for visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn for statistical data visualizations\n",
    "import seaborn as sns\n",
    "\n",
    "# Import recall_score for calculating sensitivity\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"Ready to begin diabetes classification analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUlmAwSf39Zb"
   },
   "source": [
    "## Step 1: Load the data in a csv file into a dataframe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOJL88y4jr_d"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://advanced-machine-learning-for-medical-data-8e1579.gitlab.io/labs/lab1/breast_cancer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXYYowkM39Zb"
   },
   "source": [
    "Display top 5 rows from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEZaYmIynjFn"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAjjDvoL39Zc"
   },
   "source": [
    "Let's find out the number of rows and columns in the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvkIa0aS39Zc"
   },
   "outputs": [],
   "source": [
    "# Display rows and columns information about the dataset\n",
    "print(f'Number of rows :', df.shape[0])\n",
    "print(f'Number of columns :', df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LOGWK2939Zc"
   },
   "source": [
    "### Basic information of all features\n",
    "Let's examine the first few patient records to understand what our data looks like. This is similar to reviewing the first few patient files in a medical study.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4ayvJ9EZPm5"
   },
   "outputs": [],
   "source": [
    "# Display basic statistical information about each feature\n",
    "print(\"=== BASIC STATISTICS FOR ALL FEATURES ===\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-d_e2X-R39Zc"
   },
   "source": [
    "##  Step 2: Identify input features (X) and target variable (y)\n",
    "In medical prediction, you separate:\n",
    "\n",
    "- Input features (X):\n",
    "\n",
    "The medical measurements you use to make predictions (like symptoms and test results)\n",
    "\n",
    "- Target variable (y):\n",
    "\n",
    "What you want to predict (diagnosis: yes or no)\n",
    "\n",
    "Dataset Columns\n",
    "\n",
    "Step 1: Decide the target (y)\n",
    "\n",
    "The column Diagnosis is the natural target.\n",
    "\n",
    "Step 2: Define input features (X)\n",
    "\n",
    "All the tumor measurements will be the input features.\n",
    "That means:\n",
    "\n",
    "- Input Features (X):\n",
    "\n",
    "'radius_mean', 'texture_mean', 'perimeter_mean','area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean','concave points_mean', 'symmetry_mean' etc.\n",
    "\n",
    "\n",
    "\n",
    "X = all measurement columns (30 features), except diagnosis and id.\n",
    "\n",
    "You remove id because any column that refers to the patients personal information like name, id, emailid, phone is of no use for the model. Including them may give wrong results.\n",
    "\n",
    "y = binary target diagnosis (B = Benign(no cancer), M = Malignant(cancerous tumor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xqsP0mG39Zc"
   },
   "source": [
    "You identify the features next. Features are the values the machine learning model learns from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXjtOASA39Zc"
   },
   "outputs": [],
   "source": [
    "#Define features and target\n",
    "X = df.drop(columns=[\"id\",\"diagnosis\"])\n",
    "y = df[\"diagnosis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vnxrm4JQWD_d"
   },
   "source": [
    "## Step 3: Split the data into Train and Test sets\n",
    "\n",
    "Training set → used to train the model\n",
    "\n",
    "Testing set → used to evaluate the model's performance on unseen data\n",
    "\n",
    "Example : If X has 100 patient records and y has 100 labels:\n",
    "\n",
    "X_train → 80 rows\n",
    "\n",
    "X_test → 20 rows\n",
    "\n",
    "y_train → 80 rows\n",
    "\n",
    "y_test → 20 rows\n",
    "\n",
    "- random_state:\n",
    "\n",
    "Controls randomness in operations like splitting data.\n",
    "\n",
    "Why it matters: Many operations in machine learning involve randomness (e.g., shuffling data before splitting). Using a fixed random_state ensures the same split every time you run the code.\n",
    "\n",
    "- stratify :\n",
    "\n",
    "Think of it as keeping the proportions balanced when splitting the data.\n",
    "\n",
    "- Purpose: Ensures the target variable distribution is preserved in both train and test sets.\n",
    "- Why it matters:\n",
    "\n",
    "If your dataset is imbalanced (e.g., 80% no diabetes, 20% diabetes), random splitting might accidentally put most positive cases in either train or test — this can bias your model.\n",
    "Example: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "Here, if 20% of your patients have diabetes in the full dataset, then 20% of the test set and 20% of the training set will also have diabetes. Keeps the class distribution similar across splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below line splits your dataset into training and testing sets, which is a standard step before training a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4nPw7LP39Zi"
   },
   "outputs": [],
   "source": [
    "# 4. Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                                    X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=df.diagnosis\n",
    "                                                    )\n",
    "\n",
    "# 1. Print number of samples\n",
    "print(\"Number of training samples:\", X_train.shape[0])\n",
    "print(\"Number of test samples:\", X_test.shape[0])\n",
    "\n",
    "# 2. Optional: Check number of features\n",
    "print(\"Number of features:\", X_train.shape[1])\n",
    "\n",
    "# 3. Check class distribution in train/test sets\n",
    "print(\"\\nTraining set class distribution:\\n\", y_train.value_counts())\n",
    "print(\"\\nTest set class distribution:\\n\", y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnVaDtf539Zi"
   },
   "source": [
    "### Train the model\n",
    "Now you will train the machine learning model using the training data. This is like teaching a medical diagnostic system using historical patient records and their known outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHekSjSv58Nw"
   },
   "source": [
    "## Step 4: Create a Random Forest model with the specified parameters\n",
    "\n",
    "- n_estimators=100\n",
    "\n",
    "The number of decision trees in the forest.\n",
    "\n",
    "Each tree makes a prediction, and the forest combines them (majority vote for classification).\n",
    "\n",
    "More trees → better stability, but training takes longer.\n",
    "\n",
    "- max_depth=None\n",
    "\n",
    "This controls how deep each tree can grow.\n",
    "\n",
    "None means trees keep growing until all leaves are pure (or until other stopping conditions are met).\n",
    "\n",
    "If set smaller (e.g., max_depth=10), trees are shallower → less overfitting.\n",
    "\n",
    "- random_state=42\n",
    "\n",
    "A fixed random seed so results are reproducible.\n",
    "\n",
    "If you don't set this, results may vary each time you run the code.\n",
    "\n",
    "- n_jobs=-1\n",
    "\n",
    "Tells scikit-learn to use all CPU cores available for parallel training.\n",
    "\n",
    "This speeds up model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9nEXXBB3GR7"
   },
   "outputs": [],
   "source": [
    "# 5. Train Random Forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,      # number of trees\n",
    "    max_depth=None,        # let trees expand fully unless limited\n",
    "    random_state=42,       # for reproducibility\n",
    "    n_jobs=-1              # use all processors\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKgtqArr39Zi"
   },
   "source": [
    "## Step 5: Generate predictions and evaluate model performance\n",
    "Your model is now trained. Time to evaluate the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpMb01Pl39Zi"
   },
   "outputs": [],
   "source": [
    "# 6. Predictions using Random Forest\n",
    "predicted_values = model.predict(X_test)               # Predicted class labels\n",
    "original_values = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBoPibLEyxxV"
   },
   "source": [
    "\n",
    "### accuracy_score(y_test, y_pred)\n",
    "Compares the true labels (y_test) with the predicted labels (y_pred) from Random Forest model.\n",
    "\n",
    "Calculates the accuracy: the proportion of correctly predicted patients.\n",
    "\n",
    "Accuracy = Numberofcorrectpredictions/Totalpredictions\n",
    "\n",
    "### What classification_report does\n",
    "\n",
    "classification_report is a function from scikit-learn.\n",
    "\n",
    "It evaluates a classification model in more detail than just accuracy.\n",
    "\n",
    "It shows key metrics for each class (here: Low Risk = 0, High Risk = 1).\n",
    "\n",
    "#### Precision, Recall, F1Score, Support\n",
    "- Precision: Of all patients predicted as high risk, how many were actually high risk?\n",
    "\n",
    "    High Precision → fewer false positives\n",
    "    \n",
    "- Recall: (Sensitivity)\tOf all actual high-risk patients, how many did the model correctly identify?\tHigh Recall → fewer false negatives (important in healthcare).\n",
    "- F1-Score: Harmonic mean of precision & recall → balances both.\tUseful when data is imbalanced.\n",
    "- Support:\tNumber of actual instances of each class in y_test\n",
    "\n",
    "#### Why classification_report matters in patient risk scoring\n",
    "\n",
    "- Accuracy alone can be misleading, especially if classes are imbalanced (e.g., fewer high-risk patients).\n",
    "\n",
    "- Recall for high-risk patients is often more important clinically → you don't want to miss someone at risk.\n",
    "\n",
    "- F1-score helps balance precision and recall to get a better overall measure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cON1FEIKbsPf"
   },
   "outputs": [],
   "source": [
    "# 7. Evaluation metrics\n",
    "print(\"\\n Model Accuracy:\", accuracy_score(original_values, predicted_values))\n",
    "\n",
    "print(\"\\n Classification Report:\\n\", classification_report(original_values, predicted_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance is a technique used to identify which input variables have the greatest influence on a predictive model's decisions. In the context of the Wisconsin Breast Cancer Dataset, feature importance scores reveal which cellular characteristics—such as radius, texture, or concavity—are most critical in distinguishing between benign and malignant tumors. By ranking features based on their contribution to the model's predictive accuracy, you can gain insights into the biological factors most indicative of cancer, improve model interpretability, and potentially reduce dimensionality by focusing on the most relevant variables. Higher importance scores indicate features that play a more significant role in the classification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature importances dataframe\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': X_test.columns,\n",
    "    'importance': model.feature_importances_,\n",
    "}).sort_values('importance', ascending=False)\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create horizontal bar chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=feature_importances, y='feature', x='importance', palette='viridis')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Feature Importances', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Qxas0-L3NDr"
   },
   "source": [
    "## Step 6: Generate the confusion matrix\n",
    "confusion_matrix is from scikit-learn.\n",
    "It compares actual labels (y_test) vs predicted labels (y_pred).\n",
    "\n",
    "Returns a 2×2 matrix for binary classification (Low Risk = 0, High Risk = 1):\n",
    "##sns.heatmap() is from Seaborn to make a color-coded matrix.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "cm → the confusion matrix data\n",
    "\n",
    "annot=True → shows the numbers in each cell\n",
    "\n",
    "fmt=\"d\" → format numbers as integers\n",
    "\n",
    "xticklabels / yticklabels → label rows and columns for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_5S38ntaTOb"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(original_values, predicted_values)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\",\n",
    "            xticklabels=[\"Low Risk\", \"High Risk\"],\n",
    "            yticklabels=[\"Low Risk\", \"High Risk\"])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uww2kl3r8Xh7"
   },
   "source": [
    "## Step 7: Calculate sensitivity and specificity\n",
    "These two are super important in healthcare ML because they describe how well your model detects disease vs. avoids false alarms.\n",
    "\n",
    "#####   **Sensitivity **(a.k.a Recall, True Positive Rate)\n",
    "\n",
    "- Definition: Of all the actual positive (High Risk) patients, how many did the model correctly identify as positive?\n",
    "\n",
    "- Intuition: How good is the test at catching sick patients?\n",
    "\n",
    "- Healthcare example:\n",
    "\n",
    "   - If 100 people actually have cancer, and the model correctly detects 95, then Sensitivity = 95%.\n",
    "\n",
    "   - High Sensitivity = few missed cases (low false negatives).\n",
    "\n",
    "#####   **Specificity **(a.k.a True Negative Rate)\n",
    "\n",
    "- Definition: Of all the actual negative (Low Risk) patients, how many did the model correctly identify as negative?\n",
    "- Intuition: How good is the test at ruling out healthy patients?\n",
    "\n",
    "- Healthcare example:\n",
    "\n",
    "If 100 people are healthy, and the model correctly says 90 are healthy, then Specificity = 90%.\n",
    "\n",
    "High Specificity = few false alarms (low false positives).\n",
    "\n",
    "-------------------------------\n",
    "\n",
    " **Putting them together**:\n",
    "\n",
    "Sensitivity = “Don't miss sick patients.”\n",
    "\n",
    "Specificity = “Don't wrongly label healthy patients as sick.”\n",
    "\n",
    "### Example: Healthcare trade-off example:\n",
    "\n",
    "In cancer screening:\n",
    "\n",
    "Example with 1,000 patients\n",
    "\n",
    "- 100 patients have cancer (true positives group)\n",
    "\n",
    "- 900 patients are healthy (true negatives group)\n",
    "\n",
    "#### Case A: High Sensitivity (95%), Lower Specificity (80%)\n",
    "\n",
    "Test detects 95/100 cancer patients correctly. This means it wrongly flags 20% of healthy patients.\n",
    "\n",
    "180 people incorrectly told they may have cancer.\n",
    "\n",
    "- Outcome: Almost no cancer patient is missed, but many healthy patients face unnecessary stress and extra tests.\n",
    "\n",
    "#### Case B: High Specificity (98%), Lower Sensitivity (70%)\n",
    "\n",
    "Test detects only 70/100 cancer patients.\n",
    "\n",
    "But only 2% of healthy patients (18 people) are wrongly flagged.\n",
    "\n",
    "- Outcome: Very few healthy people are worried unnecessarily, but 30 cancer patients are missed, which can be dangerous.\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "**Takeaway**\n",
    "\n",
    "High Sensitivity → Good for screening (you want to catch everyone at risk, even if it means false alarms).\n",
    "\n",
    "High Specificity → Good for confirmation (you only want to say “yes” when you are very sure).\n",
    "\n",
    "That's why in healthcare, screening tests are usually designed to be highly sensitive, and then follow-up diagnostic tests are designed to be highly specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OctWgax5kveG"
   },
   "source": [
    "### Print the sensitivity score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2UAY6mwBgKO"
   },
   "outputs": [],
   "source": [
    "# Sensitivity (True Positive Rate) = TP / (TP + FN)\n",
    "# It measures how well the model identifies positive cases (liver disease patients)\n",
    "# pos_label parameter specifies which class is considered positive\n",
    "# Print the sensitivity score\n",
    "sensitivity = recall_score(original_values, predicted_values, pos_label='M')\n",
    "print(f\"Sensitivity Score: {sensitivity:.4f}\")\n",
    "print(f\"Sensitivity Percentage: {sensitivity * 100:.2f}%\")\n",
    "print(\"Sensitivity measures the model's ability to correctly identify liver disease patients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-8rWF1hk4Vv"
   },
   "source": [
    "### Print the specificity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wae48QnKBoR5"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Specificity (True Negative Rate) = TN / (TN + FP)\n",
    "# It measures how well the model identifies negative cases (healthy patients)\n",
    "# For specificity, you use pos_label=0 (assuming 0 represents healthy patients)\n",
    "specificity = recall_score(original_values, predicted_values,pos_label='B')\n",
    "# Print the specificity score\n",
    "print(f\"Specificity Score: {specificity:.4f}\")\n",
    "print(f\"Specificity Percentage: {specificity * 100:.2f}%\")\n",
    "print(\"Specificity measures the model's ability to correctly identify healthy patients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_a8PQi8v39Zj"
   },
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5VHBYRhlL1d"
   },
   "source": [
    "Use the dataset **Parkinson_disease.csv** (https://advanced-machine-learning-for-medical-data-8e1579.gitlab.io/labs/lab1/Parkinson_disease.csv) for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnYnAlTR39Zj"
   },
   "source": [
    "## Exercise 1: Load a dataset (Parkinson) and display first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oS0A1FvH39Zj"
   },
   "outputs": [],
   "source": [
    "# df = # your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1moxF-W39Zj"
   },
   "source": [
    "<details>\n",
    "    <summary>Click here for a hint</summary>\n",
    "    \n",
    "Use the read_csv function, shape, and head()\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFVGdR3M39Zj"
   },
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\"https://advanced-machine-learning-for-medical-data-8e1579.gitlab.io/labs/lab1/Parkinson_disease.csv\")\n",
    "print(\"Rows and columns of the dataset :\", df.shape)\n",
    "print(\"First 5 rows:\\n\", df.head())\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlqjO-3z39Zj"
   },
   "source": [
    "## Exercise 2: Identify the target column and the data columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8RpxTc-39Zj"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iT2AzK3m39Zj"
   },
   "source": [
    "<details>\n",
    "    <summary>Click here for a hint</summary>\n",
    "    \n",
    "Refer to Step 2\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfR3lrU-39Zj"
   },
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "# Target variable = 'Category'\n",
    "print(\"All columns information:\")\n",
    "df.info()\n",
    "target = \"status\"\n",
    "X = df.drop(columns=[target,'name'], errors=\"ignore\")\n",
    "y = df[target]\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Xt-0T9b39Zj"
   },
   "source": [
    "## Exercise 3: Split the data into training and test sets (Stratified 80:20)\n",
    "- Split the data\n",
    "- Display number of samples in both training and test cases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CFXAL5s39Zj"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiHOcwET39Zj"
   },
   "source": [
    "<details>\n",
    "    <summary>Click here for a hint</summary>\n",
    "    \n",
    "Refer to Step 3\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJjHmLbS39Zj"
   },
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "#Split the dataset in the ratio of 80:20\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, stratify=y, random_state=42 )\n",
    "#print the samples\n",
    "print(\"Number of training samples:\", X_train.shape[0])\n",
    "print(\"Number of test samples:\", X_test.shape[0])\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Pcod0o539Zj"
   },
   "source": [
    "## Exercise 4: Train the model on training data using Random Forest classifier. (Use n_estimators=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vOqRe1p39Zj"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14sesqF439Zk"
   },
   "source": [
    "<details>\n",
    "    <summary>Click here for a hint</summary>\n",
    "    \n",
    "  Refer to Step 4\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbxfL5iG39Zk"
   },
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "# Train the model on training data\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200,      # number of trees\n",
    "    max_depth=None,        # let trees expand fully unless limited\n",
    "    random_state=42,       # for reproducibility\n",
    "    n_jobs=-1              # use all processors\n",
    ")\n",
    "\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HW83n0Zx39Zk"
   },
   "source": [
    "## Exercise 5: Predict using the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_1Y0Sdr39Zk"
   },
   "outputs": [],
   "source": [
    " # your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scEWUQiD39Zk"
   },
   "source": [
    "<details>\n",
    "    <summary>Click here for a hint</summary>\n",
    "    \n",
    "use the predict() method\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMB9eVO539Zk"
   },
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "#  Predictions using Random Forest\n",
    "y_pred = rf_clf.predict(X_test)    \n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_a6-of14Q8l"
   },
   "source": [
    "## Exercise 6: Evaluate the model using accuracy and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_CwT_Yz43Cc"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLaWjnc3VvV7"
   },
   "source": [
    "<details>\n",
    "    <summary>Click here for a hint</summary>\n",
    "    \n",
    "Refer to Step 6\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp6Dn4gfV26I"
   },
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "#  Evaluation metrics\n",
    "print(\"\\n Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ap0bGm4RWeJl"
   },
   "source": [
    "## Exercise 7: Find the sensitivity and display it in percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEDTojhHWpbI"
   },
   "outputs": [],
   "source": [
    "#your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOBGRzf8WtGm"
   },
   "source": [
    "<details>\n",
    "    <summary>Click here for a hint</summary>\n",
    "    \n",
    "Refer to Step 7\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvo-bHHPW1yc"
   },
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Sensitivity\n",
    "sensitivity = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "print(f\"Sensitivity Percentage: {sensitivity * 100:.2f}%\") \n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99cKkz2I39Zk"
   },
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have successfully completed this lab on **Basic Supervised Model Implementation for Patient Risk Scoring** using the Wisconsin Breast Cancer dataset and a Random Forest classifier. You learned how to prepare and split a clinical dataset, train a Random Forest model, evaluate its performance using key metrics, and interpret results for clinical decision-making. These skills form the foundation for building reliable and interpretable machine learning models in healthcare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27cm5wP239Zk"
   },
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjkoi0Jq39Zk"
   },
   "source": [
    "Ramesh Sannareddy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxR2CUt239Zk"
   },
   "source": [
    "Copyright © 2025 SkillUp. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "prev_pub_hash": "3295c58449e10e36934fbfb7240d185e0a1cdc184f3987613a24470680c1d8f3"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
