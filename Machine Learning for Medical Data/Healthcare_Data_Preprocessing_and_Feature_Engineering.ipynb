{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Healthcare Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time estimate: **30** minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    " - Identify and handle common data quality issues including missing values, duplicates, inconsistent entries, and outliers\n",
    " - Remove Personally Identifiable Information (PII) to ensure data privacy compliance with HIPAA and GDPR\n",
    " - Apply data transformation techniques including standardization, normalization, and encoding for machine learning\n",
    " - Engineer domain-specific features from healthcare data to improve model performance\n",
    "\n",
    "## What you will do in this lab\n",
    "\n",
    "In this hands-on lab, you will work with real-world healthcare data that contains typical quality issues found in medical datasets. You'll learn to clean and prepare this data for machine learning applications while maintaining patient privacy.\n",
    "\n",
    "You will:\n",
    "\n",
    "- Explore and identify data quality issues in a healthcare dataset\n",
    "- Remove sensitive patient information (PII) to comply with privacy regulations\n",
    "- Handle missing values using appropriate imputation strategies\n",
    "- Standardize inconsistent categorical data and mixed units\n",
    "- Detect and handle outliers using statistical methods\n",
    "- Engineer meaningful features such as BMI and temporal indicators\n",
    "- Encode categorical variables and scale numeric features for ML readiness\n",
    "\n",
    "## Overview\n",
    "\n",
    "Data preprocessing is a critical step in any machine learning pipeline, but it becomes especially important in healthcare applications where data quality directly impacts patient outcomes. Raw healthcare data often contains inconsistencies, missing values, mixed formats, and privacy-sensitive information that must be carefully addressed before building predictive models.\n",
    "\n",
    "In this lab, you'll work with a synthetic healthcare dataset that simulates real-world challenges such as inconsistent gender labels (M/Male/F/Female), mixed measurement units (lbs/kg), various date formats, and missing diagnostic information. You'll learn systematic approaches to clean this data while maintaining its utility for analysis.\n",
    "\n",
    "The preprocessing pipeline you'll build follows industry best practices: first removing PII for privacy, then addressing data quality issues, followed by feature engineering to create more informative variables, and finally transforming the data into a format suitable for machine learning algorithms. These skills are directly applicable to real healthcare analytics projects where clean, privacy-compliant data is essential.\n",
    "\n",
    "By the end of this lab, you'll have a complete understanding of how to transform messy healthcare data into a clean, standardized dataset ready for predictive modeling tasks such as risk assessment or disease diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset\n",
    "\n",
    "This lab uses a synthetic healthcare dataset designed to simulate real-world medical data challenges.\n",
    "\n",
    "### Dataset overview\n",
    "\n",
    "The dataset contains patient health records including demographics, vital measurements, diagnostic information, and risk indicators. This data simulates what you might encounter in electronic health records (EHR) systems, complete with the messiness and inconsistencies typical of real medical data. The dataset includes 200 patient records with intentionally introduced quality issues such as missing values, inconsistent formatting, mixed units, and duplicate entries to provide realistic preprocessing practice.\n",
    "\n",
    "### Column descriptions\n",
    "\n",
    "1. **Patient_ID** - Unique identifier for each patient (e.g., P016, P_new_126)\n",
    "2. **Age** - Age of the patient in years (may contain missing values or outliers)\n",
    "3. **Gender** - Gender of the patient (inconsistent formats: M, Male, F, Female, Other)\n",
    "4. **Ethnicity** - Ethnic background of the patient (Asian, African, Caucasian, Hispanic with inconsistent capitalization)\n",
    "5. **Weight** - Weight of the patient in mixed units (kg or lbs, e.g., 70, 150lbs)\n",
    "6. **Height_cm** - Height of the patient in centimeters (numeric values)\n",
    "7. **Diagnosis_Date** - Date when diagnosis was made (multiple date formats: YYYY-MM-DD, DD/MM/YYYY)\n",
    "8. **Diagnosis_Code** - Medical diagnosis code or abbreviation (DEP=Depression, OCD=Obsessive Compulsive Disorder, ANX=Anxiety, ANXITY=typo for Anxiety)\n",
    "9. **Glucose_mg_dL** - Blood glucose level in mg/dL (may indicate diabetes risk)\n",
    "10. **Risk** - Binary risk indicator (0 = low risk, 1 = high risk for adverse health outcomes)\n",
    "11. **Patient_Name** - Full name of the patient (PII - to be removed)\n",
    "12. **EmailID** - Email address of the patient (PII - to be removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Installing required libraries\n",
    "\n",
    "The following libraries are required to run this lab. Pandas will be used for data manipulation, NumPy for numerical operations, SciPy for statistical functions, and Scikit-learn for preprocessing utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the libraries required for this lab\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scipy\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: suppress warnings for cleaner output\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   # For data loading, manipulation, cleaning, and saving (DataFrame operations)\n",
    "import numpy as np    # For numerical operations, array handling, and missing value operations\n",
    "import re             # For regular expression pattern matching in text processing\n",
    "\n",
    "from datetime import datetime, timedelta  # For parsing and manipulating date/time values\n",
    "import random  # For generating random values during data exploration\n",
    "\n",
    "from scipy import stats  # For statistical functions like z-score for outlier detection\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "# StandardScaler: standardizes features (mean=0, std=1) for ML algorithms\n",
    "# MinMaxScaler: scales features to a fixed range (typically 0-1)\n",
    "# OneHotEncoder: converts categorical features into binary indicator variables\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"Ready to begin healthcare data preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and explore the raw data\n",
    "\n",
    "Before cleaning data, it's essential to understand what you're working with. In this step, you'll load the healthcare dataset and perform an initial exploration to identify data quality issues. This exploration phase helps you make informed decisions about which preprocessing techniques to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw healthcare data from CSV file\n",
    "df = pd.read_csv(\"https://advanced-machine-learning-for-medical-data-8e1579.gitlab.io/labs/lab2/raw_data.csv\")\n",
    "\n",
    "# Display the first few rows to get an initial sense of the data\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understand data quality issues\n",
    "\n",
    "Real-world healthcare data commonly suffers from four main quality issues:\n",
    "\n",
    "1. **Missing data**: Important fields left blank or null\n",
    "2. **Duplicates**: Identical records appearing multiple times\n",
    "3. **Inconsistent entries**: Same category with different labels (e.g., M vs Male)\n",
    "4. **Outliers**: Extreme or impossible values (e.g., Age=200, Glucose=500)\n",
    "\n",
    "Let's systematically identify these issues in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset structure\n",
    "print(\"Dataset dimensions:\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "print(\"\\nColumn names and data types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "print((df.isnull().sum() / len(df) * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "dup_rows = df.duplicated(keep=False)\n",
    "print(f\"\\nNumber of duplicate rows: {dup_rows.sum()}\")\n",
    "if dup_rows.any():\n",
    "    display(df[dup_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify inconsistent categorical entries\n",
    "print(\"\\nUnique values in Gender column:\")\n",
    "print(df['Gender'].unique())\n",
    "print(\"\\nUnique values in Ethnicity column:\")\n",
    "print(df['Ethnicity'].unique())\n",
    "print(\"\\nUnique values in Diagnosis_Code column:\")\n",
    "print(df['Diagnosis_Code'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for mixed units in Weight column\n",
    "print(\"\\nUnique Weight values (showing mixed units):\")\n",
    "display(df['Weight'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary to identify potential outliers\n",
    "print(\"\\nStatistical summary of numeric columns:\")\n",
    "display(df[['Age', 'Glucose_mg_dL']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check date format inconsistencies\n",
    "print(\"\\nSample of Diagnosis_Date values (showing mixed formats):\")\n",
    "display(df['Diagnosis_Date'].sample(10, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance for the target variable\n",
    "print(\"\\nRisk value distribution:\")\n",
    "print(df['Risk'].value_counts())\n",
    "print(\"\\nRisk percentage distribution:\")\n",
    "print((df['Risk'].value_counts() / len(df) * 100).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Detect outliers using statistical methods\n",
    "\n",
    "Outliers can significantly impact machine learning models. You'll use two common statistical methods to detect them:\n",
    "\n",
    "### Interquartile Range (IQR) method\n",
    "- **Formula**: IQR = Q3 − Q1 (difference between 75th and 25th percentiles)\n",
    "- **Outlier definition**: Values below Q1 − 1.5 × IQR or above Q3 + 1.5 × IQR\n",
    "- **Best for**: Non-normally distributed data (robust against skewness)\n",
    "\n",
    "### Z-Score method\n",
    "- **Formula**: Z = (Value − Mean) / Standard Deviation\n",
    "- **Outlier definition**: |Z-score| > 3 (more than 3 standard deviations from mean)\n",
    "- **Best for**: Normally distributed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect outliers using IQR method\n",
    "def iqr_outliers(series):\n",
    "    \"\"\"\n",
    "    Detect outliers using the Interquartile Range (IQR) method.\n",
    "    \n",
    "    Parameters:\n",
    "    series: pandas Series - numeric column to check for outliers\n",
    "    \n",
    "    Returns:\n",
    "    pandas Series - containing only the outlier values\n",
    "    \"\"\"\n",
    "    q1 = series.quantile(0.25)  # 25th percentile\n",
    "    q3 = series.quantile(0.75)  # 75th percentile\n",
    "    iqr = q3 - q1                # Interquartile range\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return series[(series < lower_bound) | (series > upper_bound)]\n",
    "\n",
    "# Detect outliers in Age\n",
    "print(\"Age outliers (IQR method):\")\n",
    "age_outliers = iqr_outliers(df['Age'].dropna())\n",
    "print(f\"Found {len(age_outliers)} outliers in Age\")\n",
    "print(f\"Outlier values: {age_outliers.unique()}\")\n",
    "\n",
    "# Detect outliers in Glucose\n",
    "print(\"\\nGlucose outliers (IQR method):\")\n",
    "glucose_outliers = iqr_outliers(df['Glucose_mg_dL'].dropna())\n",
    "print(f\"Found {len(glucose_outliers)} outliers in Glucose_mg_dL\")\n",
    "if len(glucose_outliers) > 0:\n",
    "    print(f\"Outlier values: {glucose_outliers.unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create a clean copy and remove PII\n",
    "\n",
    "Privacy protection is paramount in healthcare data. **Personally Identifiable Information (PII)** includes any data that can directly or indirectly identify an individual. Common PII in healthcare includes:\n",
    "\n",
    "- **Direct identifiers**: Patient names, email addresses, phone numbers, addresses\n",
    "- **Semi-identifiers**: Patient IDs (can be kept if properly anonymized)\n",
    "- **Sensitive dates**: Birth dates, exact diagnosis dates (often generalized)\n",
    "\n",
    "Regulations like **HIPAA** (USA) and **GDPR** (Europe) require removing or anonymizing PII before data analysis or sharing.\n",
    "\n",
    "You'll create a copy of the original data (to preserve the raw data) and remove PII columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy - keep original data untouched for reference\n",
    "df_clean = df.copy()\n",
    "print(\"Working copy created. Original data preserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and remove PII columns\n",
    "pii_columns = ['Patient_Name', 'EmailID']\n",
    "print(f\"Removing PII columns: {pii_columns}\")\n",
    "\n",
    "df_clean = df_clean.drop(columns=pii_columns, errors='ignore')\n",
    "\n",
    "print(\"\\nColumns after removing PII:\")\n",
    "print(df_clean.columns.tolist())\n",
    "print(f\"\\nReduced from {len(df.columns)} to {len(df_clean.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Remove duplicate rows\n",
    "\n",
    "Duplicate records can occur due to data entry errors, system glitches, or merging datasets. They can:\n",
    "- Bias analysis by overrepresenting certain patients\n",
    "- Inflate dataset size artificially\n",
    "- Cause data leakage in train-test splits\n",
    "\n",
    "You'll identify and remove exact duplicate rows, keeping only the first occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count duplicates before removal\n",
    "duplicates_before = df_clean.duplicated().sum()\n",
    "rows_before = len(df_clean)\n",
    "\n",
    "# Remove exact duplicate rows (keep first occurrence)\n",
    "df_clean = df_clean.drop_duplicates(keep='first')\n",
    "\n",
    "# Report results\n",
    "rows_after = len(df_clean)\n",
    "print(f\"Rows before deduplication: {rows_before}\")\n",
    "print(f\"Duplicate rows found: {duplicates_before}\")\n",
    "print(f\"Rows after deduplication: {rows_after}\")\n",
    "print(f\"Rows removed: {rows_before - rows_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Standardize inconsistent categorical variables\n",
    "\n",
    "Inconsistent categorical data is common in healthcare due to:\n",
    "- Multiple data entry personnel with different conventions\n",
    "- Data merging from different systems\n",
    "- Typos and abbreviations\n",
    "\n",
    "You need to standardize:\n",
    "- **Gender**: Convert M/Male/m → 'Male', F/Female/f → 'Female'\n",
    "- **Ethnicity**: Standardize capitalization (asian/Asian/ASIAN → 'Asian')\n",
    "- **Diagnosis_Code**: Fix typos and standardize (OCD/ocd → 'OCD', ANXITY → 'ANX')\n",
    "\n",
    "This ensures categorical data is **clean, consistent, and machine-readable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize Gender column\n",
    "print(\"Before standardization - Gender unique values:\")\n",
    "print(df_clean['Gender'].value_counts(dropna=False))\n",
    "\n",
    "# Convert to lowercase and strip whitespace for consistent matching\n",
    "df_clean['Gender'] = df_clean['Gender'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Define mapping for known variations\n",
    "gender_map = {\n",
    "    'male': 'Male', 'm': 'Male',\n",
    "    'female': 'Female', 'f': 'Female',\n",
    "    'other': 'Other',\n",
    "    'nan': np.nan, 'none': np.nan\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "df_clean['Gender'] = df_clean['Gender'].replace({'nan': np.nan})\n",
    "df_clean['Gender'] = df_clean['Gender'].map(\n",
    "    lambda x: gender_map.get(x, x.capitalize() if pd.notna(x) else x)\n",
    ")\n",
    "\n",
    "print(\"\\nAfter standardization - Gender unique values:\")\n",
    "print(df_clean['Gender'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize Ethnicity column\n",
    "print(\"Before standardization - Ethnicity unique values:\")\n",
    "print(df_clean['Ethnicity'].value_counts(dropna=False))\n",
    "\n",
    "# Standardize capitalization\n",
    "df_clean['Ethnicity'] = df_clean['Ethnicity'].astype(str).str.strip()\n",
    "df_clean['Ethnicity'] = df_clean['Ethnicity'].replace({'nan': np.nan})\n",
    "df_clean['Ethnicity'] = df_clean['Ethnicity'].where(\n",
    "    df_clean['Ethnicity'].isna(),\n",
    "    df_clean['Ethnicity'].str.capitalize()\n",
    ")\n",
    "\n",
    "print(\"\\nAfter standardization - Ethnicity unique values:\")\n",
    "print(df_clean['Ethnicity'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize Diagnosis_Code column\n",
    "print(\"Before standardization - Diagnosis_Code unique values:\")\n",
    "print(df_clean['Diagnosis_Code'].value_counts(dropna=False))\n",
    "\n",
    "# Convert to uppercase and fix common typos\n",
    "df_clean['Diagnosis_Code'] = df_clean['Diagnosis_Code'].astype(str).str.strip().str.upper()\n",
    "df_clean['Diagnosis_Code'] = df_clean['Diagnosis_Code'].replace({\n",
    "    'NAN': np.nan,\n",
    "    'ANXITY': 'ANX',  # Fix typo\n",
    "    'OCD.': 'OCD'      # Remove trailing period\n",
    "})\n",
    "\n",
    "print(\"\\nAfter standardization - Diagnosis_Code unique values:\")\n",
    "print(df_clean['Diagnosis_Code'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing categorical values with 'Unknown'\n",
    "df_clean['Gender'] = df_clean['Gender'].fillna('Unknown')\n",
    "df_clean['Ethnicity'] = df_clean['Ethnicity'].fillna('Unknown')\n",
    "df_clean['Diagnosis_Code'] = df_clean['Diagnosis_Code'].fillna('Unknown')\n",
    "\n",
    "print(\"Missing categorical values filled with 'Unknown'\")\n",
    "print(\"\\nFinal categorical value counts:\")\n",
    "print(\"\\nGender:\")\n",
    "print(df_clean['Gender'].value_counts())\n",
    "print(\"\\nEthnicity:\")\n",
    "print(df_clean['Ethnicity'].value_counts())\n",
    "print(\"\\nDiagnosis_Code:\")\n",
    "print(df_clean['Diagnosis_Code'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Normalize mixed units and engineer BMI feature\n",
    "\n",
    "Healthcare data often contains mixed measurement units due to different countries or systems using different standards (metric vs imperial). You need to:\n",
    "\n",
    "1. **Normalize Weight**: Convert all weights to kg (from mixed kg and lbs)\n",
    "2. **Convert Height**: Convert cm to meters for BMI calculation\n",
    "3. **Engineer BMI**: Body Mass Index is a clinically important derived feature\n",
    "\n",
    "**BMI Formula**: BMI = Weight(kg) / Height(m)²\n",
    "\n",
    "**BMI Categories**:\n",
    "- Underweight: < 18.5\n",
    "- Normal: 18.5 - 24.9\n",
    "- Overweight: 25 - 29.9\n",
    "- Obese: ≥ 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert weight to kg (handles both numeric kg and string 'lbs' format)\n",
    "def weight_to_kg(x):\n",
    "    \"\"\"\n",
    "    Convert weight to kilograms.\n",
    "    Handles numeric values (assumed kg) and strings with 'lbs' suffix.\n",
    "    \n",
    "    Examples:\n",
    "    70 -> 70.0 kg\n",
    "    '150lbs' -> 68.04 kg\n",
    "    '150 lbs' -> 68.04 kg\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    \n",
    "    # If already numeric, assume it's in kg\n",
    "    if isinstance(x, (int, float, np.integer, np.floating)):\n",
    "        return float(x)\n",
    "    \n",
    "    # Handle string values\n",
    "    s = str(x).strip().lower()\n",
    "    \n",
    "    # Check for lbs pattern (e.g., '150lbs' or '150 lbs')\n",
    "    match = re.match(r'^\\s*([0-9]+(?:\\.[0-9]+)?)\\s*lbs?\\s*$', s)\n",
    "    if match:\n",
    "        lbs = float(match.group(1))\n",
    "        return round(lbs * 0.45359237, 2)  # Convert lbs to kg\n",
    "    \n",
    "    # Try to parse as numeric (assume kg)\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Apply weight conversion\n",
    "df_clean['Weight_kg'] = df_clean['Weight'].apply(weight_to_kg)\n",
    "\n",
    "print(\"Weight conversion examples:\")\n",
    "display(df_clean[['Weight', 'Weight_kg']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert height from cm to meters\n",
    "df_clean['Height_cm'] = pd.to_numeric(df_clean['Height_cm'], errors='coerce')\n",
    "df_clean['Height_m'] = df_clean['Height_cm'] / 100.0\n",
    "\n",
    "print(\"Height converted from cm to meters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BMI (Body Mass Index)\n",
    "df_clean['BMI'] = df_clean.apply(\n",
    "    lambda row: round(row['Weight_kg'] / (row['Height_m'] ** 2), 2)\n",
    "    if pd.notna(row['Weight_kg']) and pd.notna(row['Height_m']) and row['Height_m'] > 0\n",
    "    else np.nan,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"BMI calculated successfully\")\n",
    "print(\"\\nSample of engineered features:\")\n",
    "display(df_clean[['Weight', 'Weight_kg', 'Height_cm', 'Height_m', 'BMI']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Handle missing values with imputation\n",
    "\n",
    "Missing values are inevitable in healthcare data. Common causes include:\n",
    "- Tests not performed for all patients\n",
    "- Data entry errors\n",
    "- Equipment failures\n",
    "- Patient privacy restrictions\n",
    "\n",
    "### Why use Median instead of Mean?\n",
    "\n",
    "For healthcare data, **median imputation** is often preferred over mean because:\n",
    "\n",
    "1. **Robust to outliers**: Healthcare data often contains extreme values (very high glucose, unusual ages)\n",
    "2. **Mean is sensitive**: A few extreme values can skew the mean significantly\n",
    "3. **Median represents center**: The middle value of sorted data, unaffected by extremes\n",
    "4. **Preserves distribution**: Better maintains the shape of skewed distributions\n",
    "5. **Simple and fast**: Computationally efficient with no assumptions about distribution\n",
    "\n",
    "You'll impute missing values in numeric columns (Age, Weight_kg, Height_cm, BMI, Glucose_mg_dL) using their respective medians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values before imputation\n",
    "print(\"Missing values before imputation:\")\n",
    "numeric_cols = ['Age', 'Weight_kg', 'Height_cm', 'BMI', 'Glucose_mg_dL']\n",
    "print(df_clean[numeric_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median imputation for numeric columns\n",
    "for col in numeric_cols:\n",
    "    median_val = df_clean[col].median()\n",
    "    df_clean[col] = df_clean[col].fillna(median_val)\n",
    "    print(f\"Imputed {col} with median = {median_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify imputation\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df_clean[numeric_cols].isnull().sum())\n",
    "print(\"\\nAll numeric missing values successfully imputed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Parse dates and engineer temporal features\n",
    "\n",
    "Temporal features can be highly informative in healthcare:\n",
    "- **Diagnosis year**: May reflect changes in diagnostic practices or disease prevalence\n",
    "- **Time since diagnosis**: Important for understanding disease progression\n",
    "- **Seasonal patterns**: Some conditions vary by time of year\n",
    "\n",
    "You'll parse the inconsistent date formats and extract useful temporal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates with mixed formats (YYYY-MM-DD and DD/MM/YYYY)\n",
    "df_clean['Diagnosis_Date_parsed'] = pd.to_datetime(\n",
    "    df_clean['Diagnosis_Date'],\n",
    "    errors='coerce',  # Convert unparseable dates to NaT (Not a Time)\n",
    "    dayfirst=True     # Assume day comes first in ambiguous formats\n",
    ")\n",
    "\n",
    "print(\"Date parsing results:\")\n",
    "print(f\"Successfully parsed: {df_clean['Diagnosis_Date_parsed'].notna().sum()} dates\")\n",
    "print(f\"Failed to parse: {df_clean['Diagnosis_Date_parsed'].isna().sum()} dates\")\n",
    "\n",
    "print(\"\\nSample of original vs parsed dates:\")\n",
    "display(df_clean[['Diagnosis_Date', 'Diagnosis_Date_parsed']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year from diagnosis date\n",
    "df_clean['Diagnosis_Year'] = df_clean['Diagnosis_Date_parsed'].dt.year\n",
    "\n",
    "# Calculate days since diagnosis (relative to most recent date in dataset)\n",
    "ref_date = df_clean['Diagnosis_Date_parsed'].max()\n",
    "if pd.isna(ref_date):\n",
    "    ref_date = pd.to_datetime(\"today\")\n",
    "\n",
    "df_clean['Days_Since_Diagnosis'] = (\n",
    "    ref_date - df_clean['Diagnosis_Date_parsed']\n",
    ").dt.days\n",
    "\n",
    "print(f\"\\nReference date for calculating time since diagnosis: {ref_date.date()}\")\n",
    "print(\"\\nSample of engineered temporal features:\")\n",
    "display(df_clean[['Diagnosis_Date_parsed', 'Diagnosis_Year', 'Days_Since_Diagnosis']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Encode categorical variables\n",
    "\n",
    "Machine learning algorithms require numeric input. Categorical variables must be converted to numbers through **encoding**.\n",
    "\n",
    "### One-hot encoding\n",
    "\n",
    "One-hot encoding creates **binary (0/1) columns** for each category:\n",
    "\n",
    "**Example**: If Diagnosis_Code has values ['DEP', 'OCD', 'ANX']\n",
    "- Creates columns: `Diagnosis_Code_DEP`, `Diagnosis_Code_OCD`, `Diagnosis_Code_ANX`\n",
    "- A patient with 'OCD' gets: [0, 1, 0]\n",
    "\n",
    "**Why use one-hot encoding?**\n",
    "- Treats all categories equally (no implicit ordering)\n",
    "- Works with all ML algorithms\n",
    "- Prevents models from assuming numerical relationships between categories\n",
    "\n",
    "**Alternative**: Label Encoding (1, 2, 3...) should only be used for ordinal data with natural ordering.\n",
    "\n",
    "You'll apply one-hot encoding to Gender, Ethnicity, and Diagnosis_Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to categorical columns\n",
    "print(\"Columns before encoding:\")\n",
    "print(df_clean.columns.tolist())\n",
    "print(f\"Total columns: {len(df_clean.columns)}\")\n",
    "\n",
    "df_final = pd.get_dummies(\n",
    "    df_clean,\n",
    "    columns=['Diagnosis_Code', 'Gender', 'Ethnicity'],\n",
    "    drop_first=False  # Keep all columns (set True to drop one for linear models)\n",
    ")\n",
    "\n",
    "print(\"\\nColumns after encoding:\")\n",
    "print(df_final.columns.tolist())\n",
    "print(f\"Total columns: {len(df_final.columns)}\")\n",
    "print(f\"\\nNew encoded columns created: {len(df_final.columns) - len(df_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the encoded dataset\n",
    "print(\"Sample of encoded data:\")\n",
    "display(df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Scale numeric features\n",
    "\n",
    "### Why scale features?\n",
    "\n",
    "Many machine learning algorithms are sensitive to feature scale:\n",
    "- **Example**: Age (range 0-100) vs Glucose (range 70-500)\n",
    "- Without scaling, algorithms may give more importance to features with larger values\n",
    "- Algorithms affected: Logistic Regression, SVM, KNN, Neural Networks, K-Means\n",
    "- Algorithms NOT affected: Tree-based models (Decision Trees, Random Forest, XGBoost)\n",
    "\n",
    "### StandardScaler (Z-score normalization)\n",
    "\n",
    "**Formula**: z = (x - μ) / σ\n",
    "- Transforms data to have **mean = 0** and **standard deviation = 1**\n",
    "- **Best for**: Algorithms assuming normal distribution (Linear/Logistic Regression, SVM)\n",
    "- **Range**: Typically between -3 and +3 (but unbounded)\n",
    "\n",
    "You'll apply StandardScaler to all numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numeric columns to scale\n",
    "numeric_cols_to_scale = ['Age', 'Weight_kg', 'Height_cm', 'BMI', 'Glucose_mg_dL', 'Days_Since_Diagnosis']\n",
    "\n",
    "# Filter to only existing columns\n",
    "numeric_cols_existing = [col for col in numeric_cols_to_scale if col in df_final.columns]\n",
    "\n",
    "print(f\"Scaling {len(numeric_cols_existing)} numeric features:\")\n",
    "print(numeric_cols_existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any remaining missing values with median before scaling\n",
    "df_final[numeric_cols_existing] = df_final[numeric_cols_existing].fillna(\n",
    "    df_final[numeric_cols_existing].median()\n",
    ")\n",
    "\n",
    "print(\"Verified no missing values before scaling:\")\n",
    "print(df_final[numeric_cols_existing].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_columns = [col + '_scaled' for col in numeric_cols_existing]\n",
    "df_final[scaled_columns] = scaler.fit_transform(df_final[numeric_cols_existing])\n",
    "\n",
    "print(\"Scaling complete!\")\n",
    "print(\"\\nScaled features statistics (should have mean≈0, std≈1):\")\n",
    "display(df_final[scaled_columns].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs scaled values\n",
    "print(\"\\nComparison of original vs scaled values:\")\n",
    "comparison_cols = ['Age', 'Age_scaled', 'Glucose_mg_dL', 'Glucose_mg_dL_scaled']\n",
    "display(df_final[comparison_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Save the cleaned dataset\n",
    "\n",
    "Now that you've completed all preprocessing steps, you'll save the cleaned dataset to a CSV file. This file is now ready for:\n",
    "- Exploratory data analysis (EDA)\n",
    "- Machine learning model training\n",
    "- Statistical analysis\n",
    "- Sharing with team members (with PII removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset\n",
    "output_path = \"healthcare_cleaned_data.csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✓ Cleaned dataset saved to: {output_path}\")\n",
    "print(f\"\\nFinal dataset shape: {df_final.shape[0]} rows × {df_final.shape[1]} columns\")\n",
    "print(f\"Original dataset shape: {df.shape[0]} rows × {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display working directory\n",
    "import os\n",
    "print(f\"\\nFile saved in directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Evaluate data cleaning results\n",
    "\n",
    "Let's compare the raw and cleaned datasets to verify preprocessing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare column structures\n",
    "print(\"=\"*60)\n",
    "print(\"COLUMN COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nRaw data columns ({len(df.columns)}):\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"\\nCleaned data columns ({len(df_clean.columns)}):\")\n",
    "print(df_clean.columns.tolist())\n",
    "print(f\"\\nFinal encoded data columns ({len(df_final.columns)}):\")\n",
    "print(df_final.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare missing values\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING VALUES COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nRaw data missing values:\")\n",
    "print(df.isna().sum())\n",
    "print(f\"\\nTotal missing values in raw data: {df.isna().sum().sum()}\")\n",
    "\n",
    "print(\"\\nCleaned data missing values:\")\n",
    "print(df_clean.isna().sum())\n",
    "print(f\"\\nTotal missing values in cleaned data: {df_clean.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare statistical summaries\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICAL SUMMARY COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nRaw data summary:\")\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\nCleaned data summary:\")\n",
    "display(df_clean.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare categorical standardization (Gender example)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CATEGORICAL STANDARDIZATION - GENDER EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nRaw Gender value counts:\")\n",
    "print(df['Gender'].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nCleaned Gender value counts:\")\n",
    "print(df_clean['Gender'].value_counts(dropna=False))\n",
    "\n",
    "print(\" Successfully standardized from 6 variations to 4 consistent categories!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Exercises\n",
    "\n",
    "Now it's your turn! Apply what you've learned to a new synthetic healthcare dataset. The following exercises will test your understanding of the data preprocessing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Load and prepare data\n",
    "\n",
    "Load the **synthetic_data.csv** file (https://advanced-machine-learning-for-medical-data-8e1579.gitlab.io/labs/lab2/synthetic_data.csv) into a DataFrame and create a clean working copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a hint</summary>\n",
    "    \n",
    "Use the `read_csv()` function to load the data, then use `.copy()` to create a working copy. Reference **Step 1** for the exact syntax.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "# Load the synthetic healthcare data\n",
    "df = pd.read_csv(\"https://advanced-machine-learning-for-medical-data-8e1579.gitlab.io/labs/lab2/synthetic_data.csv\")\n",
    "\n",
    "# Create a working copy\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Display column names to verify\n",
    "print(\"Columns in dataset:\")\n",
    "print(df_clean.columns.tolist())\n",
    "print(f\"\\nDataset loaded: {df_clean.shape[0]} rows × {df_clean.shape[1]} columns\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Remove personal data\n",
    "\n",
    "Identify and remove all PII (Personally Identifiable Information) columns from the dataset. Common PII includes: Patient_ID, Name, Address, Phone, Email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a hint</summary>\n",
    "    \n",
    "Use the `.drop()` method with `columns` parameter. Set `errors='ignore'` to avoid errors if a column doesn't exist. Reference **Step 4** for the syntax.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "# Define PII columns to remove\n",
    "pii_cols = ['Patient_ID', 'Name', 'Address', 'Phone', 'Email']\n",
    "\n",
    "# Remove PII columns (only if they exist)\n",
    "df_clean = df_clean.drop(\n",
    "    columns=[col for col in pii_cols if col in df_clean.columns],\n",
    "    errors='ignore'\n",
    ")\n",
    "\n",
    "print(\"After removing PII columns:\")\n",
    "print(df_clean.columns.tolist())\n",
    "print(f\"\\nColumns remaining: {len(df_clean.columns)}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Drop duplicate rows\n",
    "\n",
    "Check for and remove any duplicate rows in the dataset. Report how many duplicates were found and removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a hint</summary>\n",
    "    \n",
    "Use `.drop_duplicates()` method with `keep='first'` parameter. Count rows before and after to see how many were removed. Reference **Step 5**.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "# Count rows before deduplication\n",
    "rows_before = len(df_clean)\n",
    "\n",
    "# Remove exact duplicate rows (keep first occurrence)\n",
    "df_clean = df_clean.drop_duplicates(keep='first')\n",
    "\n",
    "# Count rows after deduplication\n",
    "rows_after = len(df_clean)\n",
    "\n",
    "# Report results\n",
    "print(f\"Rows before deduplication: {rows_before}\")\n",
    "print(f\"Rows after deduplication: {rows_after}\")\n",
    "print(f\"Duplicate rows removed: {rows_before - rows_after}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Handle missing values in numeric columns\n",
    "\n",
    "Identify all numeric columns, check for missing values, and impute them using the median strategy. Verify that all missing values have been filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a hint</summary>\n",
    "    \n",
    "First, use `.select_dtypes(include=[np.number])` to get numeric columns. Then use `.median()` and `.fillna()` for each column. Reference **Step 8** for the complete approach.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "# Identify numeric columns\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Numeric columns found: {numeric_cols}\")\n",
    "\n",
    "# Check missing values before imputation\n",
    "print(\"\\nMissing values before imputation:\")\n",
    "print(df_clean[numeric_cols].isnull().sum())\n",
    "\n",
    "# Convert to numeric (coerce invalid values to NaN)\n",
    "for col in numeric_cols:\n",
    "    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Median imputation\n",
    "for col in numeric_cols:\n",
    "    median_val = df_clean[col].median()\n",
    "    df_clean[col] = df_clean[col].fillna(median_val)\n",
    "    print(f\"Imputed {col} with median = {median_val}\")\n",
    "\n",
    "# Verify\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df_clean[numeric_cols].isnull().sum())\n",
    "print(\"\\n✓ All numeric missing values imputed successfully!\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Standardize categorical variables\n",
    "\n",
    "Standardize the inconsistent categorical entries in the `Gender` and `Disease_Type` columns. Print the unique values before and after standardization.\n",
    "\n",
    "**Hint**: Gender variations might include: M/Male/m/F/Female/f  \n",
    "**Hint**: Disease_Type variations might include: ckd/CKD, LD/ld/Liver Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a hint</summary>\n",
    "    \n",
    "Use `.str.lower()` and `.str.strip()` first, then create a mapping dictionary to standardize variations. Reference **Step 6** for the complete pattern.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "# Standardize Gender\n",
    "print(\"Before standardization - Gender:\")\n",
    "print(df_clean['Gender'].value_counts(dropna=False))\n",
    "\n",
    "df_clean['Gender'] = df_clean['Gender'].astype(str).str.strip().str.lower()\n",
    "gender_map = {\n",
    "    'male': 'Male', 'm': 'Male',\n",
    "    'female': 'Female', 'f': 'Female',\n",
    "    'other': 'Other',\n",
    "    'nan': np.nan, 'none': np.nan\n",
    "}\n",
    "df_clean['Gender'] = df_clean['Gender'].replace({'nan': np.nan})\n",
    "df_clean['Gender'] = df_clean['Gender'].map(\n",
    "    lambda x: gender_map.get(x, x.capitalize() if pd.notna(x) else x)\n",
    ")\n",
    "\n",
    "print(\"\\nAfter standardization - Gender:\")\n",
    "print(df_clean['Gender'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Congratulations!\n",
    "\n",
    "You have successfully completed this lab on healthcare data preprocessing! You've learned how to systematically clean messy real-world data by handling missing values, removing duplicates, standardizing inconsistent entries, engineering meaningful features, and preparing data for machine learning. These skills are essential for any data science project, especially in healthcare where data quality directly impacts patient outcomes and model reliability.\n",
    "\n",
    "## Authors\n",
    "\n",
    "Ramesh Sannareddy\n",
    "\n",
    "Copyright © 2025 SkillUp. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
