{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Rare Medical Conditions with Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time estimate: **30** minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    " - Identify and analyze class imbalance in medical datasets\n",
    " - Apply Synthetic Minority Oversampling Technique (SMOTE) to handle imbalanced data\n",
    " - Evaluate models using Precision, Recall, and Precision-Recall curves\n",
    " - Describe why accuracy is misleading for rare conditions\n",
    " - Build effective classifiers for detecting rare medical conditions\n",
    "\n",
    "## What you will do in this lab\n",
    "\n",
    "In medical diagnostics, detecting rare conditions presents unique challenges. When a disease affects only 1-2% of patients, traditional machine learning approaches often fail. A model that simply predicts \"healthy\" for everyone would achieve 98-99% accuracy while being completely useless in practice.\n",
    "\n",
    "This lab addresses the real-world challenge of building reliable diagnostic tools for rare conditions. You will work with a highly imbalanced medical dataset where malignant cases are significantly less common than benign cases, mirroring actual clinical scenarios.\n",
    "\n",
    "You will:\n",
    "\n",
    "- Analyze a highly imbalanced medical dataset with rare positive cases\n",
    "- Train baseline machine learning models and identify their limitations\n",
    "- Apply Synthetic Minority Oversampling Technique (SMOTE) to balance the training data\n",
    "- Compare model performance using appropriate metrics for imbalanced data\n",
    "- Learn to interpret Precision-Recall curves for clinical decision-making\n",
    "\n",
    "## Overview\n",
    "\n",
    "Class imbalance is one of the most common challenges in medical machine learning. When positive cases (patients with a condition) represent less than 5% of the dataset, standard classification algorithms become biased toward the majority class. This creates a dangerous situation in medical diagnostics: models may achieve impressive accuracy scores while failing to detect the very cases they were designed to identify.\n",
    "\n",
    "The problem stems from how most machine learning algorithms optimize their objective functions. They minimize overall error rate, which in an imbalanced dataset means correctly classifying the abundant negative cases. The algorithm \"learns\" that predicting \"negative\" most of the time yields good performance metrics, even if this strategy misses all positive cases.\n",
    "\n",
    "This lab introduces you to techniques specifically designed for imbalanced classification problems. You will use SMOTE, a sophisticated resampling technique that creates synthetic examples of the minority class by interpolating between existing positive cases. This approach is superior to simple duplication because it helps the model learn the broader decision boundary of the positive class without overfitting to specific examples.\n",
    "\n",
    "Beyond resampling techniques, you will learn to evaluate models using metrics appropriate for imbalanced data. Precision-Recall curves, F1-scores, and careful analysis of confusion matrices provide insights that accuracy alone cannot reveal. These evaluation methods help data scientists and medical professionals make informed decisions about model deployment, threshold selection, and the trade-offs between false positives and false negatives in clinical settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset\n",
    "\n",
    "This lab uses a synthetic medical dataset designed to simulate the challenge of detecting rare conditions in clinical practice.\n",
    "\n",
    "### Dataset overview\n",
    "\n",
    "The dataset contains medical measurements from 1,000 patients, where malignant cases represent approximately 1% of the total samples. This extreme imbalance (99:1 ratio) reflects realistic scenarios in medical screening and diagnostics, where most patients do not have the condition being tested.\n",
    "\n",
    "The synthetic nature of this dataset allows for safe learning and experimentation without privacy concerns, while maintaining the statistical properties and challenges found in real medical data. Each patient is represented by 10 numerical features derived from medical measurements, along with a binary target indicating the diagnosis.\n",
    "\n",
    "### Column descriptions\n",
    "\n",
    "1. **feature_1** - Normalized medical measurement (continuous value)\n",
    "2. **feature_2** - Normalized medical measurement (continuous value)\n",
    "3. **feature_3** - Normalized medical measurement (continuous value)\n",
    "4. **feature_4** - Normalized medical measurement (continuous value)\n",
    "5. **feature_5** - Normalized medical measurement (continuous value)\n",
    "6. **feature_6** - Normalized medical measurement (continuous value)\n",
    "7. **feature_7** - Normalized medical measurement (continuous value)\n",
    "8. **feature_8** - Normalized medical measurement (continuous value)\n",
    "9. **feature_9** - Normalized medical measurement (continuous value)\n",
    "10. **feature_10** - Normalized medical measurement (continuous value)\n",
    "11. **target** - Diagnosis (0 = Benign/Healthy, 1 = Malignant/Condition Present)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Installing required libraries\n",
    "\n",
    "The following libraries are required to run this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the libraries required for this lab\n",
    "!pip install -q imblearn\n",
    "!pip install -q pandas\n",
    "!pip install -q numpy\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "!pip install -q scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: suppress warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data manipulation and numerical operations\n",
    "import pandas as pd  # for data manipulation and analysis\n",
    "import numpy as np  # for numerical operations\n",
    "\n",
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt  # for creating plots\n",
    "import seaborn as sns  # for statistical visualizations\n",
    "\n",
    "# Import tools for splitting data into training and test sets\n",
    "from sklearn.model_selection import train_test_split  # for data splitting\n",
    "\n",
    "# Import resampling techniques to handle imbalanced data\n",
    "from imblearn.over_sampling import SMOTE  # Synthetic Minority Over-sampling Technique\n",
    "\n",
    "# Import the classifier (machine learning model)\n",
    "from sklearn.ensemble import RandomForestClassifier  # ensemble learning model\n",
    "\n",
    "# Import evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report,  # detailed classification metrics\n",
    "    confusion_matrix,  # matrix showing prediction outcomes\n",
    "    precision_recall_curve,  # precision-recall trade-off visualization\n",
    "    average_precision_score,  # summary metric for precision-recall\n",
    "    roc_auc_score,  # area under ROC curve\n",
    "    roc_curve,  # receiver operating characteristic curve\n",
    "    accuracy_score  # simple accuracy metric\n",
    ")\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"Ready to begin rare condition detection analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility (ensures consistent results across runs)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plot style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Environment configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and explore the dataset\n",
    "\n",
    "The first step in any machine learning project is to load and explore the data. Understanding the structure, size, and basic characteristics of your dataset is crucial before applying any algorithms. In this step, you will load the imbalanced medical dataset and examine its basic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imbalanced dataset from CSV file\n",
    "df = pd.read_csv(\"https://advanced-machine-learning-for-medical-data-8e1579.gitlab.io/labs/lab3/imbalanced_dataset.csv\")\n",
    "\n",
    "# Display the first few rows to understand the data structure\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display dataset dimensions\n",
    "print(\"\\nDataset shape:\", df.shape)\n",
    "print(f\"This means there are {df.shape[0]} samples (patients) and {df.shape[1]-1} features (measurements)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Analyze class distribution\n",
    "\n",
    "Understanding the class distribution is critical when working with medical datasets. In rare condition detection, the ratio of positive to negative cases (class imbalance) directly impacts model performance and evaluation strategy.\n",
    "\n",
    "An imbalance ratio of 10:1 or higher is considered severe, and special techniques are required. Medical datasets often have ratios of 100:1 or even 1000:1, making this analysis essential for choosing appropriate modeling approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of cases for each class\n",
    "class_counts = df['target'].value_counts()\n",
    "print(\"Class distribution (counts):\")\n",
    "print(class_counts)\n",
    "\n",
    "# Calculate the percentage of each class\n",
    "class_percentages = df['target'].value_counts(normalize=True) * 100\n",
    "print(\"\\nClass distribution (percentages):\")\n",
    "for class_label, percentage in class_percentages.items():\n",
    "    class_name = \"Benign\" if class_label == 0 else \"Malignant\"\n",
    "    print(f\"  {class_name} (Class {class_label}): {percentage:.2f}%\")\n",
    "\n",
    "# Calculate and display the imbalance ratio\n",
    "imbalance_ratio = class_counts[0] / class_counts[1]\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "print(f\"This means there are approximately {round(imbalance_ratio)} benign cases for every 1 malignant case.\")\n",
    "print(\"\\n This severe imbalance will cause problems for standard machine learning algorithms!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the class distribution with a bar chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "class_counts.plot(kind='bar', color=['green', 'red'])\n",
    "plt.title('Distribution of Diagnoses', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Diagnosis (0 = Benign, 1 = Malignant)', fontsize=12)\n",
    "plt.ylabel('Number of Cases', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels on bars for clarity\n",
    "for i, (count, pct) in enumerate(zip(class_counts, class_percentages)):\n",
    "    plt.text(i, count + 10, f'{pct:.1f}%', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare the data for machine learning\n",
    "\n",
    "Before training a model, you must split the data into features (X) and the target variable (y), then divide it into training and testing sets. The training set is used to teach the model, while the test set evaluates how well the model generalizes to unseen data.\n",
    "\n",
    "The `stratify` parameter ensures both sets maintain the same class distribution as the original dataset. This is especially important for imbalanced data, as it prevents the test set from accidentally having zero positive cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) from the target variable (y)\n",
    "X = df.drop(columns=['target'])  # All measurement columns\n",
    "y = df['target']  # Only the diagnosis column\n",
    "\n",
    "# Split into training set (80%) and test set (20%)\n",
    "# stratify=y ensures both sets have similar class distributions\n",
    "# random_state=42 ensures reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Data split completed!\")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "print(\"\\n‚úì Both sets maintain the severe class imbalance from the original dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train a baseline model (without resampling)\n",
    "\n",
    "Before applying any techniques to handle class imbalance, it's important to establish a baseline. This baseline model is trained on the original imbalanced data without any special handling. By comparing this baseline to models trained with resampling techniques, you can measure the effectiveness of your imbalance-handling strategies.\n",
    "\n",
    "Random Forest is an ensemble learning method that combines multiple decision trees. It's chosen here because it generally performs well on various tasks and serves as a strong baseline classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest classifier\n",
    "# n_estimators=100 means it will create 100 decision trees\n",
    "# random_state=42 ensures reproducibility\n",
    "baseline_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the imbalanced training data\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "\n",
    "# Get probability scores (used for precision-recall curves)\n",
    "# [:, 1] selects probabilities for the positive class (malignant)\n",
    "y_pred_proba_baseline = baseline_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"‚úì Baseline model trained successfully!\")\n",
    "print(\"Next: Evaluate this model and see why accuracy can be misleading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the baseline model\n",
    "\n",
    "Now comes the critical analysis: evaluating how well the baseline model performs. This step reveals a fundamental problem with imbalanced datasets: traditional metrics like accuracy can be highly misleading.\n",
    "\n",
    "A model that achieves 99% accuracy sounds excellent, but if it never predicts the positive class (malignant), it's completely useless for medical diagnosis. This is why you must examine multiple metrics, especially Precision, Recall, and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "baseline_accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE MODEL EVALUATION (No Resampling)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nAccuracy: {baseline_accuracy:.3f} ({baseline_accuracy*100:.1f}%)\")\n",
    "print(\"\\n  WARNING: High accuracy can be misleading with imbalanced data!\")\n",
    "print(\"   A model that predicts 'benign' for ALL cases would have ~99% accuracy.\")\n",
    "print(\"   Let's look at more detailed metrics...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_baseline, target_names=['Benign', 'Malignant'], zero_division=0))\n",
    "\n",
    "print(\"\\n Key metrics explained:\")\n",
    "print(\"  ‚Ä¢ Precision: Of cases predicted as malignant, what % are actually malignant?\")\n",
    "print(\"  ‚Ä¢ Recall: Of actual malignant cases, what % was detected?\")\n",
    "print(\"  ‚Ä¢ F1-Score: Harmonic mean of precision and recall\")\n",
    "print(\"\\n  For rare condition detection, RECALL is often most critical!\")\n",
    "print(\"  Missing a malignant case (false negative) can be life-threatening.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display confusion matrix\n",
    "cm_baseline = confusion_matrix(y_test, y_pred_baseline)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_baseline)\n",
    "\n",
    "# Extract confusion matrix components\n",
    "tn, fp, fn, tp = cm_baseline.ravel()\n",
    "\n",
    "print(\"\\n Confusion Matrix Breakdown:\")\n",
    "print(f\"  True Negatives (TN): {tn}\")\n",
    "print(f\"    ‚Üí Correctly identified benign cases ‚úì\")\n",
    "print(f\"\\n  False Positives (FP): {fp}\")\n",
    "print(f\"    ‚Üí Benign cases incorrectly predicted as malignant\")\n",
    "print(f\"    ‚Üí Causes unnecessary worry and follow-up tests\")\n",
    "print(f\"\\n  False Negatives (FN): {fn}\")\n",
    "print(f\"    ‚Üí Malignant cases incorrectly predicted as benign\")\n",
    "print(f\"    ‚Üí MOST DANGEROUS ERROR - missed diagnoses!\")\n",
    "print(f\"\\n  True Positives (TP): {tp}\")\n",
    "print(f\"    ‚Üí Correctly identified malignant cases ‚úì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Benign', 'Malignant'],\n",
    "            yticklabels=['Benign', 'Malignant'],\n",
    "            cbar=False)\n",
    "plt.title('Confusion Matrix - Baseline Model', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual Diagnosis', fontsize=12)\n",
    "plt.xlabel('Predicted Diagnosis', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Notice: The bottom-left cell (False Negatives) shows missed malignant cases.\")\n",
    "print(\"  This is the error you want to minimize most in medical diagnosis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Apply SMOTE resampling\n",
    "\n",
    "Now you will apply SMOTE to address the class imbalance. SMOTE is more sophisticated than simple duplication because it creates synthetic examples by interpolating between existing minority class samples.\n",
    "\n",
    "**How SMOTE works:**\n",
    "1. For each minority class sample, SMOTE finds its k nearest neighbors\n",
    "2. It randomly selects one of these neighbors\n",
    "3. It creates a new synthetic sample along the line connecting the two samples\n",
    "4. This process repeats until the classes are balanced\n",
    "\n",
    "This approach helps the model learn the broader characteristics of the minority class without simply memorizing specific examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to the training data\n",
    "# SMOTE creates synthetic minority class examples by interpolating between existing ones\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"SMOTE Resampling Applied\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\nOriginal training set class distribution:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "\n",
    "print(\"\\nAfter SMOTE class distribution:\")\n",
    "print(pd.Series(y_train_smote).value_counts())\n",
    "\n",
    "print(\"\\n‚úì Classes are now balanced!\")\n",
    "print(f\"  Training set size increased from {len(y_train)} to {len(y_train_smote)} samples\")\n",
    "print(f\"  Added {len(y_train_smote) - len(y_train)} synthetic malignant cases\")\n",
    "\n",
    "print(\"\\n Important: SMOTE is applied only to the TRAINING set.\")\n",
    "print(\" The TEST set remains unchanged to provide unbiased evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train model with SMOTE-resampled data\n",
    "\n",
    "With the balanced training data created by SMOTE, you can now train a new model. This model should learn to recognize both classes more effectively because it sees equal numbers of positive and negative examples during training.\n",
    "\n",
    "Remember: the test set remains unchanged. You want to evaluate how well the model performs on real, unbalanced data that reflects actual clinical conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with SMOTE-resampled data\n",
    "model_smote = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_smote = model_smote.predict(X_test)\n",
    "\n",
    "# Get probability scores for precision-recall curves\n",
    "y_pred_proba_smote = model_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Model trained with SMOTE-resampled data!\")\n",
    "print(\"Next: Compare this model's performance to the baseline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Compare model performance\n",
    "\n",
    "Now comes the moment of truth: comparing the baseline model (trained on imbalanced data) with the SMOTE model (trained on balanced data). You'll calculate Precision, Recall, F1-score, and Accuracy for both models.\n",
    "\n",
    "**Metric definitions:**\n",
    "- **Accuracy** = (TP + TN) / Total ‚Äî Overall correctness, but misleading for imbalanced data\n",
    "- **Precision** = TP / (TP + FP) ‚Äî Of predicted positives, how many are correct?\n",
    "- **Recall** = TP / (TP + FN) ‚Äî Of actual positives, how many were detected?\n",
    "- **F1-score** = 2 √ó (Precision √ó Recall) / (Precision + Recall) ‚Äî Harmonic mean balancing both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to manually compute metrics\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"Compute precision, recall, F1-score, and accuracy from predictions.\"\"\"\n",
    "    # Convert to numpy arrays for easier computation\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate confusion matrix elements\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))  # True Positives\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))  # True Negatives\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))  # False Positives\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))  # False Negatives\n",
    "    \n",
    "    # Apply metric formulas\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for both models\n",
    "baseline_metrics = compute_metrics(y_test, y_pred_baseline)\n",
    "smote_metrics = compute_metrics(y_test, y_pred_smote)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Model': ['Baseline (No Resampling)', 'SMOTE'],\n",
    "    'Accuracy': [baseline_metrics[0], smote_metrics[0]],\n",
    "    'Precision': [baseline_metrics[1], smote_metrics[1]],\n",
    "    'Recall': [baseline_metrics[2], smote_metrics[2]],\n",
    "    'F1-Score': [baseline_metrics[3], smote_metrics[3]]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Display formatted comparison\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(f\"  ‚Ä¢ Baseline recall: {baseline_metrics[2]:.3f} ‚Üí SMOTE recall: {smote_metrics[2]:.3f}\")\n",
    "print(f\"    Improvement: {(smote_metrics[2] - baseline_metrics[2]):.3f}\")\n",
    "print(\"\\n  ‚Ä¢ SMOTE significantly improves recall (detection of malignant cases)\")\n",
    "print(\"  ‚Ä¢ This is the most important metric for rare condition detection!\")\n",
    "print(\"  ‚Ä¢ Accuracy may decrease slightly, but we're catching more cases that matter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualize confusion matrices\n",
    "\n",
    "Visual comparison of confusion matrices provides immediate insight into how each model performs. Pay special attention to the false negative (bottom-left) cell, which represents missed malignant cases, the most critical error in medical diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices for both models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "models_data = [\n",
    "    ('Baseline (No Resampling)', y_pred_baseline),\n",
    "    ('SMOTE', y_pred_smote)\n",
    "]\n",
    "\n",
    "for idx, (title, predictions) in enumerate(models_data):\n",
    "    ax = axes[idx]\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Benign', 'Malignant'],\n",
    "                yticklabels=['Benign', 'Malignant'],\n",
    "                cbar=False)\n",
    "    \n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Actual Diagnosis', fontsize=11)\n",
    "    ax.set_xlabel('Predicted Diagnosis', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observation:\")\n",
    "print(\"   Compare the bottom-left cell (False Negatives) in each matrix.\")\n",
    "print(\"   This represents missed malignant cases‚Äîthe most dangerous type of error!\")\n",
    "print(\"   SMOTE typically reduces false negatives, improving patient safety.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Plot Precision-Recall curves\n",
    "\n",
    "The **Precision-Recall curve** is the most important evaluation tool for imbalanced datasets. It shows the trade-off between Precision and Recall at different classification thresholds.\n",
    "\n",
    "**Why Precision-Recall curves are crucial:**\n",
    "- They focus on the minority class (malignant cases), which is what matters most\n",
    "- They reveal performance across all possible decision thresholds\n",
    "- They're more informative than ROC curves when classes are highly imbalanced\n",
    "- They help clinicians understand trade-offs between catching all cases vs. minimizing false alarms\n",
    "\n",
    "**Interpretation:**\n",
    "- **X-axis (Recall)**: What percentage of malignant cases does the model detect?\n",
    "- **Y-axis (Precision)**: Of cases predicted as malignant, what percentage are correct?\n",
    "- **Curves closer to top-right**: Better overall performance\n",
    "- **Average Precision (AP)**: Summary metric‚Äîhigher is better (1.0 is perfect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot precision-recall curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Store model predictions for plotting\n",
    "all_predictions = [\n",
    "    ('Baseline', y_pred_proba_baseline, 'blue'),\n",
    "    ('SMOTE', y_pred_proba_smote, 'green')\n",
    "]\n",
    "\n",
    "# Plot precision-recall curve for each model\n",
    "for name, y_pred_proba, color in all_predictions:\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    plt.plot(recall, precision, \n",
    "             label=f'{name} (AP = {avg_precision:.3f})', \n",
    "             color=color, linewidth=2.5)\n",
    "\n",
    "# Add reference line for random classifier\n",
    "no_skill = len(y_test[y_test == 1]) / len(y_test)\n",
    "plt.plot([0, 1], [no_skill, no_skill], \n",
    "         linestyle='--', color='gray', \n",
    "         label=f'Random Classifier (AP = {no_skill:.3f})', linewidth=2)\n",
    "\n",
    "plt.xlabel('Recall (Sensitivity)', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Precision', fontsize=13, fontweight='bold')\n",
    "plt.title('Precision-Recall Curves: Model Comparison', fontsize=15, fontweight='bold')\n",
    "plt.legend(loc='best', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHow to Read This Chart:\")\n",
    "print(\"  ‚Ä¢ X-axis (Recall): What percentage of malignant cases does the model detect?\")\n",
    "print(\"  ‚Ä¢ Y-axis (Precision): Of the cases predicted as malignant, what percentage are correct?\")\n",
    "print(\"  ‚Ä¢ AP (Average Precision): Summary metric‚Äîhigher is better (1.0 is perfect)\")\n",
    "print(\"  ‚Ä¢ Curves closer to the top-right corner indicate better performance\")\n",
    "print(\"\\nClinical Interpretation:\")\n",
    "print(\"  ‚Ä¢ High Recall (right side): Catch most malignant cases (fewer missed diagnoses)\")\n",
    "print(\"  ‚Ä¢ High Precision (top): Fewer false alarms (don't worry patients unnecessarily)\")\n",
    "print(\"  ‚Ä¢ The challenge: Improving one often decreases the other!\")\n",
    "print(\"  ‚Ä¢ The optimal threshold depends on the clinical context and consequences of errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Plot ROC curves (for comparison)\n",
    "\n",
    "While Precision-Recall curves are more appropriate for imbalanced data, Receiver Operating Characteristic (ROC) curves are also commonly used in medical diagnostics. This step plots ROC curves for comparison, helping you understand why Precision-Recall curves are preferred for rare conditions.\n",
    "\n",
    "**Key difference:**\n",
    "- **ROC curves** use False Positive Rate (FPR), which includes True Negatives in the denominator\n",
    "- When negative cases vastly outnumber positive cases, FPR changes very slowly\n",
    "- This makes ROC curves appear overly optimistic for imbalanced datasets\n",
    "- **Precision-Recall curves** focus exclusively on the positive class, providing more realistic assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot ROC curve for each model\n",
    "for name, y_pred_proba, color in all_predictions:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    plt.plot(fpr, tpr, \n",
    "             label=f'{name} (AUC = {auc_score:.3f})', \n",
    "             color=color, linewidth=2.5)\n",
    "\n",
    "# Add diagonal reference line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], \n",
    "         linestyle='--', color='gray', \n",
    "         label='Random Classifier (AUC = 0.500)', linewidth=2)\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=13, fontweight='bold')\n",
    "plt.title('ROC Curves: Model Comparison', fontsize=15, fontweight='bold')\n",
    "plt.legend(loc='best', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚öñÔ∏è  ROC vs Precision-Recall Curves:\")\n",
    "print(\"  ‚Ä¢ ROC curves can appear overly optimistic with imbalanced data\")\n",
    "print(\"  ‚Ä¢ They give equal weight to both classes\")\n",
    "print(\"  ‚Ä¢ False Positive Rate includes True Negatives, which dominate in imbalanced data\")\n",
    "print(\"  ‚Ä¢ Precision-Recall curves focus on the minority class (malignant)\")\n",
    "print(\"  ‚Ä¢ For rare condition detection, Precision-Recall curves are more informative!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Now it's time to apply what you've learned! These exercises will help reinforce your understanding of rare condition detection and imbalanced classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Load and analyze a new dataset\n",
    "\n",
    "Load the **imbalanced_dataset2.csv** file (https://advanced-machine-learning-for-medical-data-8e1579.gitlab.io/labs/lab3/imbalanced_dataset2.csv) and analyze its class distribution. Calculate the imbalance ratio and visualize the class distribution with a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a hint</summary>\n",
    "    \n",
    "Use `pd.read_csv()` to load the data, then use `value_counts()` to examine the target column distribution. Calculate the imbalance ratio by dividing the count of class 0 by class 1. Refer to Step 1 and Step 2 for the complete approach.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "# Load the new dataset\n",
    "df2 = pd.read_csv(\"https://advanced-machine-learning-for-medical-data-8e1579.gitlab.io/labs/lab3/imbalanced_dataset2.csv\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"First 5 rows of the new dataset:\")\n",
    "print(df2.head())\n",
    "\n",
    "# Analyze class distribution\n",
    "class_counts2 = df2['target'].value_counts()\n",
    "print(\"\\nClass distribution:\")\n",
    "print(class_counts2)\n",
    "\n",
    "# Calculate percentages\n",
    "class_percentages2 = df2['target'].value_counts(normalize=True) * 100\n",
    "print(\"\\nClass distribution (percentages):\")\n",
    "print(class_percentages2)\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "imbalance_ratio2 = class_counts2[0] / class_counts2[1]\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio2:.2f}:1\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 5))\n",
    "class_counts2.plot(kind='bar', color=['green', 'red'])\n",
    "plt.title('Distribution of Diagnoses - Dataset 2', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Diagnosis (0 = Benign, 1 = Malignant)', fontsize=12)\n",
    "plt.ylabel('Number of Cases', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (count, pct) in enumerate(zip(class_counts2, class_percentages2)):\n",
    "    plt.text(i, count + 10, f'{pct:.1f}%', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Train a baseline model on the new dataset\n",
    "\n",
    "Using the dataset from Exercise 1, split the data into training and test sets (80/20 split with stratification), train a Random Forest baseline model, and evaluate it using a confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a hint</summary>\n",
    "    \n",
    "Follow Steps 3, 4, and 5 from the lab. Separate features from the target, use `train_test_split()` with `stratify=y`, create a `RandomForestClassifier`, fit it on the training data, and evaluate with `confusion_matrix()` and `classification_report()`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "# Prepare the data\n",
    "X2 = df2.drop(columns=['target'])\n",
    "y2 = df2['target']\n",
    "\n",
    "# Split the data\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    X2, y2, test_size=0.2, random_state=42, stratify=y2\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train2.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test2.shape[0]} samples\")\n",
    "\n",
    "# Train baseline model\n",
    "baseline_model2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "baseline_model2.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_baseline2 = baseline_model2.predict(X_test2)\n",
    "y_pred_proba_baseline2 = baseline_model2.predict_proba(X_test2)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nBaseline Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test2, y_pred_baseline2):.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test2, y_pred_baseline2, target_names=['Benign', 'Malignant'], zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "cm2 = confusion_matrix(y_test2, y_pred_baseline2)\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm2, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Benign', 'Malignant'],\n",
    "            yticklabels=['Benign', 'Malignant'],\n",
    "            cbar=False)\n",
    "plt.title('Confusion Matrix - Baseline Model (Dataset 2)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Apply SMOTE and compare results\n",
    "\n",
    "Apply SMOTE to the training data from Exercise 2, train a new Random Forest model, and create a comparison table showing Accuracy, Precision, Recall, and F1-score for both the baseline and SMOTE models. Visualize the confusion matrices side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a hint</summary>\n",
    "    \n",
    "Follow Steps 6, 7, 8, and 9. Use `SMOTE()` to resample the training data, train a new model on the balanced data, compute metrics using the `compute_metrics()` function, and create comparison visualizations using matplotlib and seaborn.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "# Apply SMOTE\n",
    "smote2 = SMOTE(random_state=42)\n",
    "X_train_smote2, y_train_smote2 = smote2.fit_resample(X_train2, y_train2)\n",
    "\n",
    "print(\"After SMOTE:\")\n",
    "print(pd.Series(y_train_smote2).value_counts())\n",
    "\n",
    "# Train SMOTE model\n",
    "model_smote2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_smote2.fit(X_train_smote2, y_train_smote2)\n",
    "\n",
    "# Predictions\n",
    "y_pred_smote2 = model_smote2.predict(X_test2)\n",
    "y_pred_proba_smote2 = model_smote2.predict_proba(X_test2)[:, 1]\n",
    "\n",
    "# Compute metrics\n",
    "baseline_metrics2 = compute_metrics(y_test2, y_pred_baseline2)\n",
    "smote_metrics2 = compute_metrics(y_test2, y_pred_smote2)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data2 = {\n",
    "    'Model': ['Baseline', 'SMOTE'],\n",
    "    'Accuracy': [baseline_metrics2[0], smote_metrics2[0]],\n",
    "    'Precision': [baseline_metrics2[1], smote_metrics2[1]],\n",
    "    'Recall': [baseline_metrics2[2], smote_metrics2[2]],\n",
    "    'F1-Score': [baseline_metrics2[3], smote_metrics2[3]]\n",
    "}\n",
    "\n",
    "comparison_df2 = pd.DataFrame(comparison_data2)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON (Dataset 2)\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df2.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Visualize confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "models_data2 = [\n",
    "    ('Baseline', y_pred_baseline2),\n",
    "    ('SMOTE', y_pred_smote2)\n",
    "]\n",
    "\n",
    "for idx, (title, predictions) in enumerate(models_data2):\n",
    "    ax = axes[idx]\n",
    "    cm = confusion_matrix(y_test2, predictions)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Benign', 'Malignant'],\n",
    "                yticklabels=['Benign', 'Malignant'],\n",
    "                cbar=False)\n",
    "    \n",
    "    ax.set_title(f'{title} (Dataset 2)', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Actual', fontsize=11)\n",
    "    ax.set_xlabel('Predicted', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObserve how SMOTE improves recall (detection of malignant cases)!\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Plot Precision-Recall curves\n",
    "\n",
    "Create Precision-Recall curves for both the baseline and SMOTE models from Exercise 3. Compare the Average Precision scores and interpret which model performs better for rare condition detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a hint</summary>\n",
    "    \n",
    "Refer to Step 10. Use `precision_recall_curve()` and `average_precision_score()` from sklearn.metrics. Plot recall on the x-axis and precision on the y-axis. Include a reference line for a random classifier.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for solution</summary>\n",
    "\n",
    "```python\n",
    "# Plot precision-recall curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Prepare predictions\n",
    "all_predictions2 = [\n",
    "    ('Baseline', y_pred_proba_baseline2, 'blue'),\n",
    "    ('SMOTE', y_pred_proba_smote2, 'green')\n",
    "]\n",
    "\n",
    "# Plot each model\n",
    "for name, y_pred_proba, color in all_predictions2:\n",
    "    precision, recall, _ = precision_recall_curve(y_test2, y_pred_proba)\n",
    "    avg_precision = average_precision_score(y_test2, y_pred_proba)\n",
    "    \n",
    "    plt.plot(recall, precision,\n",
    "             label=f'{name} (AP = {avg_precision:.3f})',\n",
    "             color=color, linewidth=2.5)\n",
    "\n",
    "# Add reference line\n",
    "no_skill2 = len(y_test2[y_test2 == 1]) / len(y_test2)\n",
    "plt.plot([0, 1], [no_skill2, no_skill2],\n",
    "         linestyle='--', color='gray',\n",
    "         label=f'Random Classifier (AP = {no_skill2:.3f})', linewidth=2)\n",
    "\n",
    "plt.xlabel('Recall (Sensitivity)', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Precision', fontsize=13, fontweight='bold')\n",
    "plt.title('Precision-Recall Curves - Dataset 2', fontsize=15, fontweight='bold')\n",
    "plt.legend(loc='best', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe model with higher Average Precision (AP) and curve closer to top-right performs better.\")\n",
    "print(\"SMOTE typically improves performance on the minority class (malignant cases).\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have successfully completed this lab on detecting rare medical conditions with machine learning. You now understand how to identify class imbalance, apply SMOTE resampling, evaluate models using appropriate metrics, and interpret Precision-Recall curves for clinical decision-making. These skills are essential for building reliable diagnostic tools in real-world medical applications.\n",
    "\n",
    "## Authors\n",
    "\n",
    "Ramesh Sannareddy\n",
    "\n",
    "Copyright ¬© 2025 SkillUp. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
